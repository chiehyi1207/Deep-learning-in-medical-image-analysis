{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":64236,"databundleVersionId":7081163,"sourceType":"competition"}],"dockerImageVersionId":30616,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# For network processing\nimport timm\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nimport argparse\n\n# For system operation\nimport os\nimport io\nimport random\nimport datetime\nimport time\nimport glob\nfrom tqdm import tqdm\n\n# For image processing\nfrom PIL import Image\nimport cv2\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom timm.data.constants import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\n\n# For data augmentation\nimport albumentations as A\nfrom albumentations.pytorch.transforms import ToTensorV2\nfrom albumentations import *\n\n# For data processing\nimport wandb\nfrom collections import defaultdict, deque\n\n!pip install -q torchprofile\nfrom torchprofile import profile_macs","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-12-12T07:37:08.127954Z","iopub.execute_input":"2023-12-12T07:37:08.128667Z","iopub.status.idle":"2023-12-12T07:37:29.656222Z","shell.execute_reply.started":"2023-12-12T07:37:08.128619Z","shell.execute_reply":"2023-12-12T07:37:29.654970Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Utils","metadata":{}},{"cell_type":"code","source":"def seed_everything(seed):\n    # Set Python random seed\n    random.seed(seed)\n    \n    # Set NumPy random seed\n    np.random.seed(seed)\n    \n    # Set PyTorch random seed for CPU and GPU\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n    \n    # Set PyTorch deterministic operations for cudnn backend\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False","metadata":{"execution":{"iopub.status.busy":"2023-12-12T06:17:44.559797Z","iopub.execute_input":"2023-12-12T06:17:44.560287Z","iopub.status.idle":"2023-12-12T06:17:44.567733Z","shell.execute_reply.started":"2023-12-12T06:17:44.560238Z","shell.execute_reply":"2023-12-12T06:17:44.566552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def print_with_timestamp():\n    \"\"\"\n    This function add timestamp to each log info\n    \"\"\"\n    import builtins as __builtin__\n    builtin_print = __builtin__.print\n\n    def print(*args, **kwargs):\n        timestamp = datetime.datetime.now().strftime(\"[%H:%M:%S]\")\n        args = (f\"{timestamp} \",) + args\n        builtin_print(*args, **kwargs)\n\n    __builtin__.print = print","metadata":{"execution":{"iopub.status.busy":"2023-12-12T06:17:44.569385Z","iopub.execute_input":"2023-12-12T06:17:44.569827Z","iopub.status.idle":"2023-12-12T06:17:44.585845Z","shell.execute_reply.started":"2023-12-12T06:17:44.569794Z","shell.execute_reply":"2023-12-12T06:17:44.584698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Run Length Encoding\n\nRun-Length Encoding (RLE) is a simple and efficient method for representing segmented masks in computer vision and image processing. In the context of image segmentation, RLE is often used to compress and store binary masks, where each pixel is either part of the segmented object (foreground) or not (background).","metadata":{}},{"cell_type":"code","source":"def rle_encode(x, fg_val=1):\n    \"\"\"\n    Args:\n        x:  numpy array of shape (height, width), 1 - mask, 0 - background\n        fg_val: Value representing the foreground in the input array (default is 1)\n    Returns: run length encoding as list\n    \"\"\"\n    # Find indices where the array is equal to the foreground value\n    dots = np.where(x.T.flatten() == fg_val)[0]  # .T sets Fortran order down-then-right\n    \n    run_lengths = []\n    prev = -2\n    for b in dots:\n        # If the index is not consecutive, start a new run\n        if b > prev + 1:\n            run_lengths.extend((b + 1, 0))\n        run_lengths[-1] += 1  # Increment the length of the current run\n        prev = b\n\n    return run_lengths\n\ndef list_to_string(x):\n    \"\"\"\n    Converts list to a string representation\n    Empty list returns '-'\n    \"\"\"\n    if x:  # Check if the list is non-empty\n        s = str(x).replace(\"[\", \"\").replace(\"]\", \"\").replace(\",\", \"\")\n    else:\n        s = '-'\n    return s\n\ndef rle_decode(mask_rle, shape=(566, 640)):\n    '''\n    mask_rle: run-length as string formatted (start length)\n              empty predictions need to be encoded with '-'\n    shape: (height, width) of array to return \n    Returns numpy array, 1 - mask, 0 - background\n    '''\n    # Initialize an array of zeros with the specified shape\n    img = np.zeros(shape[0] * shape[1], dtype=np.uint8)\n    \n    # Check if the mask_rle is empty\n    if mask_rle != '-': \n        s = mask_rle.split()\n        # Extract start and length information from the string\n        starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n        starts -= 1  # Convert to 0-based indexing\n        ends = starts + lengths\n        \n        # Set the corresponding indices to 1 in the array\n        for lo, hi in zip(starts, ends):\n            img[lo:hi] = 1\n\n    # Reshape the array to the specified shape and return\n    return img.reshape(shape, order='F')  # Needed to align to RLE direction\n","metadata":{"execution":{"iopub.status.busy":"2023-12-12T06:17:44.589379Z","iopub.execute_input":"2023-12-12T06:17:44.589843Z","iopub.status.idle":"2023-12-12T06:17:44.605170Z","shell.execute_reply.started":"2023-12-12T06:17:44.589808Z","shell.execute_reply":"2023-12-12T06:17:44.603773Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class WandbLogger(object):\n    \n    def __init__(self, config):\n        \n        self.config = config\n        self._wandb =  wandb\n        \n    def login(self, key):\n        self._wandb.login(key=key, relogin=True)\n        \n    def init_run(self):\n        self.mask_table = wandb.Table(columns=[\"Images\", \"Masks\", \"Probs\", \"Preds\", \"Fusion\", \"IoU\"], allow_mixed_types=True)\n        self._wandb.init(\n            config=self.config,\n            project=self.config[\"project\"],\n            entity=self.config[\"entity\"],\n            name=self.config[\"name\"],\n            reinit=True\n        )\n\n    def log_checkpoints(self):\n        output_dir = self.config[\"output_dir\"]\n        model_artifact = self._wandb.Artifact(\n            self._wandb.run.id + \"_model\", type=\"model\"\n        )\n\n        model_artifact.add_dir(output_dir)\n        self._wandb.log_artifact(model_artifact, aliases=[\"latest\", \"best\"])\n        \n    def log_image(self, images, masks, probs, binary_mask_threshold, fusion, iou):\n        self.mask_table.add_data(\n            wandb.Image(images),\n            wandb.Image(masks),\n            wandb.Image(probs),\n            wandb.Image(binary_mask_threshold),\n            wandb.Image(fusion),\n            iou\n        )","metadata":{"execution":{"iopub.status.busy":"2023-12-12T06:17:44.607236Z","iopub.execute_input":"2023-12-12T06:17:44.608036Z","iopub.status.idle":"2023-12-12T06:17:44.623350Z","shell.execute_reply.started":"2023-12-12T06:17:44.607990Z","shell.execute_reply":"2023-12-12T06:17:44.622092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class SmoothedValue(object):\n    \"\"\"Track a series of values and provide access to smoothed values over a\n    window or the global series average.\n    \"\"\"\n\n    def __init__(self, window_size=20, fmt=None):\n        if fmt is None:\n            fmt = \"{value:.4f} ({global_avg:.4f})\"\n        self.deque = deque(maxlen=window_size)\n        self.total = 0.0\n        self.count = 0\n        self.fmt = fmt\n\n    def update(self, value, n=1):\n        self.deque.append(value)\n        self.count += n\n        self.total += value * n\n\n    def synchronize_between_processes(self):\n        \"\"\"\n        Warning: does not synchronize the deque!\n        \"\"\"\n        if not is_dist_avail_and_initialized():\n            return\n        t = torch.tensor([self.count, self.total], dtype=torch.float64, device='cuda')\n        dist.barrier()\n        dist.all_reduce(t)\n        t = t.tolist()\n        self.count = int(t[0])\n        self.total = t[1]\n\n    @property\n    def median(self):\n        d = torch.tensor(list(self.deque))\n        return d.median().item()\n\n    @property\n    def avg(self):\n        d = torch.tensor(list(self.deque), dtype=torch.float32)\n        return d.mean().item()\n\n    @property\n    def global_avg(self):\n        return self.total / self.count\n\n    @property\n    def max(self):\n        return max(self.deque)\n\n    @property\n    def value(self):\n        return self.deque[-1]\n\n    def __str__(self):\n        return self.fmt.format(\n            median=self.median,\n            avg=self.avg,\n            global_avg=self.global_avg,\n            max=self.max,\n            value=self.value)","metadata":{"execution":{"iopub.status.busy":"2023-12-12T06:17:44.625096Z","iopub.execute_input":"2023-12-12T06:17:44.625740Z","iopub.status.idle":"2023-12-12T06:17:44.641964Z","shell.execute_reply.started":"2023-12-12T06:17:44.625695Z","shell.execute_reply":"2023-12-12T06:17:44.640598Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MetricLogger(object):\n    def __init__(self, delimiter=\"\\t\"):\n        self.meters = defaultdict(SmoothedValue)\n        self.delimiter = delimiter\n\n    def update(self, **kwargs):\n        for k, v in kwargs.items():\n            if isinstance(v, torch.Tensor):\n                v = v.item()\n            assert isinstance(v, (float, int))\n            self.meters[k].update(v)\n\n    def __getattr__(self, attr):\n        if attr in self.meters:\n            return self.meters[attr]\n        if attr in self.__dict__:\n            return self.__dict__[attr]\n        raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n            type(self).__name__, attr))\n\n    def __str__(self):\n        loss_str = []\n        for name, meter in self.meters.items():\n            loss_str.append(\n                \"{}: {}\".format(name, str(meter))\n            )\n        return self.delimiter.join(loss_str)\n\n    def synchronize_between_processes(self):\n        for meter in self.meters.values():\n            meter.synchronize_between_processes()\n\n    def add_meter(self, name, meter):\n        self.meters[name] = meter\n\n    def log_every(self, iterable, print_freq, header=None):\n        i = 0\n        if not header:\n            header = ''\n        start_time = time.time()\n        end = time.time()\n        iter_time = SmoothedValue(fmt='{avg:.4f}')\n        data_time = SmoothedValue(fmt='{avg:.4f}')\n        space_fmt = ':' + str(len(str(len(iterable)))) + 'd'\n        if torch.cuda.is_available():\n            log_msg = self.delimiter.join([\n                header,\n                '[{0' + space_fmt + '}/{1}]',\n                'eta: {eta}',\n                '{meters}',\n                'time: {time}',\n                'data: {data}',\n                'max mem: {memory:.0f}'\n            ])\n        else:\n            log_msg = self.delimiter.join([\n                header,\n                '[{0' + space_fmt + '}/{1}]',\n                'eta: {eta}',\n                '{meters}',\n                'time: {time}',\n                'data: {data}'\n            ])\n        MB = 1024.0 * 1024.0\n        for obj in iterable:\n            data_time.update(time.time() - end)\n            yield obj\n            iter_time.update(time.time() - end)\n            if i % print_freq == 0:\n                eta_seconds = iter_time.global_avg * (len(iterable) - i)\n                eta_string = str(datetime.timedelta(seconds=int(eta_seconds)))\n                if torch.cuda.is_available():\n                    print(log_msg.format(\n                        i, len(iterable), eta=eta_string,\n                        meters=str(self),\n                        time=str(iter_time), data=str(data_time),\n                        memory=torch.cuda.max_memory_allocated() / MB))\n                else:\n                    print(log_msg.format(\n                        i, len(iterable), eta=eta_string,\n                        meters=str(self),\n                        time=str(iter_time), data=str(data_time)))\n            i += 1\n            end = time.time()\n        total_time = time.time() - start_time\n        total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n        print('{} Total time: {}'.format(header, total_time_str))","metadata":{"execution":{"iopub.status.busy":"2023-12-12T06:17:44.643866Z","iopub.execute_input":"2023-12-12T06:17:44.644233Z","iopub.status.idle":"2023-12-12T06:17:44.667176Z","shell.execute_reply.started":"2023-12-12T06:17:44.644200Z","shell.execute_reply":"2023-12-12T06:17:44.666199Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Dataset","metadata":{}},{"cell_type":"code","source":"#TO DO: try to selecct a reasonable augmentation set!!\ndef build_transform(is_train, args):\n    \"\"\"\n    Create a data transformation pipeline for image processing in deep learning tasks.\n    \"\"\"\n    t = []\n    if is_train:\n        t.append(A.Resize(args.input_size[-2], args.input_size[-1]))\n        t.append(A.Normalize(args.mean, args.std))\n        t.append(ToTensorV2())\n        return A.Compose(t)\n\n    t.append(A.Resize(args.input_size[-2], args.input_size[-1]))\n    t.append(A.Normalize(args.mean, args.std))\n    t.append(ToTensorV2())\n    return A.Compose(t)","metadata":{"execution":{"iopub.status.busy":"2023-12-12T06:17:44.668430Z","iopub.execute_input":"2023-12-12T06:17:44.669516Z","iopub.status.idle":"2023-12-12T06:17:44.681181Z","shell.execute_reply.started":"2023-12-12T06:17:44.669469Z","shell.execute_reply":"2023-12-12T06:17:44.680062Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class DeepMedical(torch.utils.data.Dataset):\n    def __init__(self, images, transforms = None):\n        self.transforms = transforms   \n        \n        self.image_paths = images\n        self.mask_paths = [image.replace('pre', 'post') + '_ROI.bmp' for image in images]\n\n        \n    def __len__(self):\n        return len(self.image_paths)\n    \n    def __getitem__(self, idx):\n        \n        image_path = self.image_paths[idx]\n        mask_path = self.mask_paths[idx]\n        \n        image = np.array(Image.open(image_path).convert('RGB'))\n        mask = np.array(Image.open(mask_path)) * 1\n\n        if self.transforms:\n            transformed = self.transforms(image=image, mask=mask)\n            image, mask = transformed['image'], transformed['mask']\n            \n        return image, mask\n    \nclass SonoDataset(torch.utils.data.Dataset):\n\n    def __init__(self, dataset, indices):\n        self.dataset = dataset\n        self.indices = indices\n\n    def __getitem__(self, idx):\n        return self.dataset[self.indices[idx]]\n\n    def __len__(self):\n        return len(self.indices)","metadata":{"execution":{"iopub.status.busy":"2023-12-12T06:17:44.683074Z","iopub.execute_input":"2023-12-12T06:17:44.683782Z","iopub.status.idle":"2023-12-12T06:17:44.695061Z","shell.execute_reply.started":"2023-12-12T06:17:44.683747Z","shell.execute_reply":"2023-12-12T06:17:44.693906Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Config class","metadata":{}},{"cell_type":"code","source":"class config:\n    \n    input_size = (8, 3, 448, 448)\n    mean = IMAGENET_DEFAULT_MEAN \n    std = IMAGENET_DEFAULT_STD\n\nimg_files = glob.glob(\"/kaggle/input/mia-hw4/sono/train/pre/*\")\nds = DeepMedical(images=img_files, transforms=build_transform(True, config))\ndl = torch.utils.data.DataLoader(ds,batch_size=16,shuffle=False,num_workers=0)\n\nplt.figure(figsize=(16,16))\nimgs,masks = next(iter(dl))\n\nfor i,(img,mask) in enumerate(zip(imgs,masks)):\n    img = ((img.permute(1,2,0)*torch.tensor(config.std) + torch.tensor(config.mean))*255.0).numpy().astype(np.uint8)\n        \n    plt.subplot(4,4,i+1)\n    plt.imshow(img,vmin=0,vmax=255)\n    plt.imshow(mask.squeeze().numpy(), alpha=0.2)\n    plt.axis('off')\n    plt.subplots_adjust(wspace=None, hspace=None)\n        \nprint(f\"Image shape: {imgs.shape}, mask shape {mask.shape}\")\n    \ndel ds,dl,imgs,masks","metadata":{"execution":{"iopub.status.busy":"2023-12-12T06:17:44.699800Z","iopub.execute_input":"2023-12-12T06:17:44.701005Z","iopub.status.idle":"2023-12-12T06:17:47.591286Z","shell.execute_reply.started":"2023-12-12T06:17:44.700958Z","shell.execute_reply":"2023-12-12T06:17:47.589827Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## FCN model","metadata":{}},{"cell_type":"code","source":"class FCN(nn.Module):\n    def __init__(self):\n        super(FCN, self).__init__()\n        \n        self.block1_conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n        self.block1_conv2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n        self.block1_pool = nn.MaxPool2d(kernel_size=2, stride=2)\n        \n        self.block2_conv1 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n        self.block2_conv2 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n        self.block2_pool = nn.MaxPool2d(kernel_size=2, stride=2)\n        \n        self.block3_conv1 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n        self.block3_conv2 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n        self.block3_conv3 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n        self.block3_pool = nn.MaxPool2d(kernel_size=2, stride=2)\n        \n        self.block4_conv1 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n        self.block4_conv2 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n        self.block4_conv3 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n        self.block4_pool = nn.MaxPool2d(kernel_size=2, stride=2)\n        \n        self.block5_conv1 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n        self.block5_conv2 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n        self.block5_conv3 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n        self.block5_pool = nn.MaxPool2d(kernel_size=2, stride=2)\n        \n        self.block5_predict = nn.Conv2d(512, 2, kernel_size=1, padding=0)\n        self.block5_up = nn.ConvTranspose2d(2, 2, kernel_size=4, stride=4, bias=False)\n        \n        self.block4_predict = nn.Conv2d(512, 2, kernel_size=1, padding=0)\n        self.block4_up = nn.ConvTranspose2d(2, 2, kernel_size=2, stride=2, bias=False)\n        \n        self.block3_predict = nn.Conv2d(256, 2, kernel_size=1, padding=0)\n        \n        self.total_upsample = nn.ConvTranspose2d(2, 2, kernel_size=8, stride=8, bias=False)\n\n    def forward(self, x):\n        block1 = self.block1_pool(self.block1_conv2(self.block1_conv1(x)))\n        #print(\"block1:\", block1.size())\n        block2 = self.block2_pool(self.block2_conv2(self.block2_conv1(block1)))\n        #print(\"block2:\", block2.size())\n        block3 = self.block3_pool(self.block3_conv3(self.block3_conv2(self.block3_conv1(block2))))\n        #print(\"block3:\", block3.size())\n        block4 = self.block4_pool(self.block4_conv3(self.block4_conv2(self.block4_conv1(block3))))\n        #print(\"block4:\", block4.size())\n        block5 = self.block5_pool(self.block5_conv3(self.block5_conv2(self.block5_conv1(block4))))\n        #print(\"block5:\", block5.size())\n        transpose5 = self.block5_up(self.block5_predict(block5))\n        #print(\"transpose5:\", transpose5.size())\n        transpose4 = self.block4_up(self.block4_predict(block4))\n        #print(\"transpose4:\", transpose4.size())\n        transpose3 = self.block3_predict(block3)\n        #print(\"transpose3:\", transpose3.size())\n        \n        addition_upsample = transpose5+transpose4+transpose3\n        output = self.total_upsample(addition_upsample)\n        \n        return output","metadata":{"execution":{"iopub.status.busy":"2023-12-12T06:17:47.592613Z","iopub.execute_input":"2023-12-12T06:17:47.592941Z","iopub.status.idle":"2023-12-12T06:17:47.614607Z","shell.execute_reply.started":"2023-12-12T06:17:47.592912Z","shell.execute_reply":"2023-12-12T06:17:47.613031Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## UNet model","metadata":{}},{"cell_type":"code","source":"class DoubleConv(nn.Module):\n    \n    def __init__(self, in_c, out_c):\n        super().__init__()\n        self.conv1 = nn.Sequential(\n            nn.Conv2d(in_c, out_c, kernel_size=3, padding=1),\n            nn.ReLU()\n        )\n        self.conv2 = nn.Sequential(\n            nn.Conv2d(out_c, out_c, kernel_size=3, padding=1),\n            nn.ReLU()\n        )\n        \n    def forward(self, x):\n        x = self.conv1(x) \n       # print(\"x size in conv1:\", x.size())\n        x = self.conv2(x) \n        #print(\"x size in conv2:\", x.size())\n        return x","metadata":{"execution":{"iopub.status.busy":"2023-12-12T06:17:47.635309Z","iopub.execute_input":"2023-12-12T06:17:47.635811Z","iopub.status.idle":"2023-12-12T06:17:47.645181Z","shell.execute_reply.started":"2023-12-12T06:17:47.635774Z","shell.execute_reply":"2023-12-12T06:17:47.643539Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Down(nn.Module):\n    def __init__(self, in_c, out_c):\n        super().__init__()\n        self.down = nn.Sequential(\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            DoubleConv(in_c,out_c),)  \n        \n    def forward(self, x):\n        x = self.down(x)\n        #print(\"x size in down:\", x.size())\n        \n        \n        return x \n    ","metadata":{"execution":{"iopub.status.busy":"2023-12-12T06:17:47.647225Z","iopub.execute_input":"2023-12-12T06:17:47.647987Z","iopub.status.idle":"2023-12-12T06:17:47.657026Z","shell.execute_reply.started":"2023-12-12T06:17:47.647942Z","shell.execute_reply":"2023-12-12T06:17:47.656135Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Up(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(Up, self).__init__()\n        self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n        self.conv = DoubleConv(in_channels, out_channels)\n\n    def forward(self, x, skip_x):\n        x = self.up(x)\n        x = torch.cat([skip_x, x], dim=1)\n        x = self.conv(x)\n        return x\n","metadata":{"execution":{"iopub.status.busy":"2023-12-12T06:17:47.658378Z","iopub.execute_input":"2023-12-12T06:17:47.659495Z","iopub.status.idle":"2023-12-12T06:17:47.672662Z","shell.execute_reply.started":"2023-12-12T06:17:47.659427Z","shell.execute_reply":"2023-12-12T06:17:47.671501Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class UNet(nn.Module):\n    def __init__(self, c_in=3, c_out=2):\n        super(UNet, self).__init__()\n        print(\"begin\")\n        self.inc = DoubleConv(c_in, 64) #(4, 3, 224, 224)\n        \n\n        self.down1 = Down(64, 128) \n       \n        self.down2 = Down(128, 256) \n        \n        self.down3 = Down(256, 256) \n       \n\n        self.bot1 = DoubleConv(256, 512) \n        self.bot2 = DoubleConv(512, 512) \n        self.bot3 = DoubleConv(512, 256) \n\n      \n        self.up1 = Up(512, 128) \n      \n        self.up2 = Up(256, 64) \n    \n        self.up3 = Up(128, 64) \n     \n\n        self.outc = nn.Conv2d(64, c_out, kernel_size=1) \n\n    \n\n    def forward(self, x):\n       \n\n        #initial conv\n        x1 = self.inc(x)\n        \n        #Down\n        x2 = self.down1(x1)\n \n        x3 = self.down2(x2)\n      \n        x4 = self.down3(x3)\n       \n\n        #Bottle neck\n        x4 = self.bot1(x4)\n        x4 = self.bot2(x4)\n        x4 = self.bot3(x4)\n\n        #Up\n        x = self.up1(x4, x3)\n        #print(\"x\",x.size())\n\n       \n        x = self.up2(x, x2)\n        #print(\"x\",x.size())\n     \n      \n        x = self.up3(x, x1)\n        #print(\"x\",x.size())\n      \n\n        #Output\n        output = self.outc(x)\n        return output","metadata":{"execution":{"iopub.status.busy":"2023-12-12T06:17:47.675537Z","iopub.execute_input":"2023-12-12T06:17:47.676071Z","iopub.status.idle":"2023-12-12T06:17:47.688958Z","shell.execute_reply.started":"2023-12-12T06:17:47.676024Z","shell.execute_reply":"2023-12-12T06:17:47.687758Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ConfusionMatrix(object):\n    \"\"\"\n    update: use to update the confusion matrix in sample-wise\n        a: ground truths\n        b: predictions\n    \n    The (i, j) terms of the matrix represents class-i pixels classify to class-j\n    \"\"\"\n    def __init__(self, num_classes):\n        self.num_classes = num_classes\n        self.mat = None\n\n    def update(self, a, b):\n        n = self.num_classes\n        if self.mat is None:\n            self.mat = torch.zeros((n, n), dtype=torch.int64, device=a.device)\n        with torch.no_grad():\n            k = (a >= 0) & (a < n)\n            inds = n * a[k].to(torch.int64) + b[k]\n            self.mat += torch.bincount(inds, minlength=n**2).reshape(n, n)\n\n    def reset(self):\n        if self.mat is not None:\n            self.mat.zero_()\n\n    def compute(self):\n        h = self.mat.float()\n        correct = torch.diag(h)\n        acc_global = correct.sum() / h.sum()\n        acc = correct / h.sum(1)\n        dice = 2 * correct  / (h.sum(1) + h.sum(0))\n        iou = correct / (h.sum(1) + h.sum(0) - correct)\n        \n        self.acc_global = acc_global\n        self.acc = acc\n        self.iou = iou\n        self.dice = dice\n        \n        return acc_global, acc, iou, dice\n\n    def reduce_from_all_processes(self):\n        if not torch.distributed.is_available():\n            return\n        if not torch.distributed.is_initialized():\n            return\n        torch.distributed.barrier()\n        torch.distributed.all_reduce(self.mat)\n\n    def __str__(self):\n        acc_global, acc, iou, dice = self.compute()\n        self.acc_global = acc_global\n        self.miou = iou.mean().item() * 100\n        self.dice = dice.mean().item() * 100\n        return (\n            'global correct: {:.2f}\\n'\n            'average row correct: {}\\n'\n            'IoU: {} mean IoU: {:.2f}\\n'\n            'dice: {} mean Dice: {:.2f}\\n').format(\n                acc_global.item() * 100,\n                ['{:.1f}'.format(i) for i in (acc * 100).tolist()],\n                ['{:.1f}'.format(i) for i in (iou * 100).tolist()],\n                iou.mean().item() * 100,\n                ['{:.2f}'.format(i) for i in (dice * 100).tolist()],\n                dice.mean().item() * 100,\n            )\n\nclass intersection_over_union:\n    \n    def __init__(self, y_true, y_pred):\n    \n        union = np.count_nonzero(y_true + y_pred)\n        intersection = np.count_nonzero(y_true * y_pred)\n        try:\n            IoU = intersection / union\n        except:\n            IoU = 0\n        self.IoU = IoU\n        self.union = (y_true + y_pred != 0) * 1\n        self.intersection = (y_true * y_pred != 0) * 1\n        self.FP = (y_true == 0) * (y_pred == 1) * 1\n        self.FN = (y_true == 1) * (y_pred == 0) * 1  \n\ndef visualization(model, valid_loader, args):\n        \n    for images, masks in valid_loader:\n    \n        images = images.to(device=args.device, dtype=torch.float)\n        masks = masks.to(device=args.device, dtype=torch.long)\n\n        logits = model(images)\n        probs = torch.nn.functional.softmax(logits, dim=1)\n\n        probs = probs[:,1,:,:]\n        preds = (probs.permute(1,2,0) * 255).cpu().detach().numpy().astype(np.uint8)\n\n        threshold = 0.5\n\n        binary_mask_threshold = (probs > threshold) * 1\n\n        masks = (masks.permute(1,2,0) * 255).cpu().numpy().astype(np.uint8)\n        images = ((images.squeeze(0).permute(1,2,0).cpu().detach().numpy()*np.array(args.std)+np.array(args.mean)) * 255).astype(np.uint8)\n        probs = (probs.permute(1,2,0) * 255).cpu().detach().numpy().astype(np.uint8)\n        binary_mask_threshold = (binary_mask_threshold.permute(1,2,0) * 255).cpu().detach().numpy().astype(np.uint8)\n\n        iou = intersection_over_union(masks/255.0, binary_mask_threshold/255.0)\n        IoU = iou.IoU\n        intersection = iou.intersection\n        union = iou.union\n        FP = iou.FP\n        FN = iou.FN\n\n        fusion = np.concatenate([FP*255, intersection*255, FN*255], axis=2).astype(np.uint8)\n        \n        yield images, masks, probs, binary_mask_threshold, fusion, IoU","metadata":{"execution":{"iopub.status.busy":"2023-12-12T06:17:47.711686Z","iopub.execute_input":"2023-12-12T06:17:47.712105Z","iopub.status.idle":"2023-12-12T06:17:47.743458Z","shell.execute_reply.started":"2023-12-12T06:17:47.712061Z","shell.execute_reply":"2023-12-12T06:17:47.742606Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train/Eval","metadata":{}},{"cell_type":"code","source":"def train_one_epoch(model, train_loader, criterion, device, epoch, optimizer, print_freq=10, num_classes=2):\n    model.train()\n    metric_logger = MetricLogger(delimiter=\"  \")\n    metric_logger.add_meter('lr', SmoothedValue(window_size=1, fmt='{value:.6f}'))\n    header = 'Epoch: [{}]'.format(epoch)\n    \n    confmat = ConfusionMatrix(num_classes)\n    for images, masks in metric_logger.log_every(train_loader, print_freq, header):\n        images = images.to(device=device, dtype=torch.float)\n        masks = masks.to(device=device, dtype=torch.long)\n\n        logits = model(images)\n        loss = criterion(logits, masks)\n        optimizer.zero_grad()\n        loss.backward()\n        \n        optimizer.step()\n    \n        confmat.update(masks.flatten(), logits.argmax(1).flatten())\n        confmat.compute()\n        \n        metric_logger.update(loss=loss.item())\n        metric_logger.update(lr=optimizer.param_groups[0][\"lr\"])\n        metric_logger.update(mIoU=torch.mean(confmat.iou[1:]))\n        metric_logger.update(Dice=torch.mean(confmat.dice[1:]))\n            \n    confmat.reduce_from_all_processes()\n\n@torch.no_grad()\ndef evaluate(model, data_loader, criterion, device, print_freq=10, num_classes=2, header=None):\n    model.eval()\n    metric_logger = MetricLogger(delimiter=\"  \")\n    header = header\n    \n    confmat = ConfusionMatrix(num_classes)\n    for images, masks in metric_logger.log_every(data_loader, print_freq, header):\n        images = images.to(device=device, dtype=torch.float)\n        masks = masks.to(device=device, dtype=torch.long)\n            \n        logits = model(images)\n        loss = criterion(logits, masks)\n\n        confmat.update(masks.flatten(), logits.argmax(1).flatten())\n        confmat.compute()\n            \n        metric_logger.update(loss=loss.item())\n        metric_logger.update(mIoU=torch.mean(confmat.iou[1:]))\n        metric_logger.update(Dice=torch.mean(confmat.dice[1:]))\n\n    confmat.reduce_from_all_processes()\n\n    return metric_logger, confmat","metadata":{"execution":{"iopub.status.busy":"2023-12-12T06:17:47.744798Z","iopub.execute_input":"2023-12-12T06:17:47.745787Z","iopub.status.idle":"2023-12-12T06:17:47.764347Z","shell.execute_reply.started":"2023-12-12T06:17:47.745744Z","shell.execute_reply":"2023-12-12T06:17:47.763159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## FCN args","metadata":{}},{"cell_type":"code","source":"#TO DO: try to set a more fitness parameters\ndef get_args_FCN():\n    parser = argparse.ArgumentParser(description='SonoDataset Training')\n\n    parser.add_argument('--root', type=str, default=\"/kaggle/input/mia-hw4/sono\")\n    parser.add_argument('--device', type=str, default=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    parser.add_argument('--batch_size', type=int, default=4)\n    parser.add_argument('--weight_decay', type=float, default=1e-3)\n    parser.add_argument('--epochs', type=int, default=70)\n    parser.add_argument('--lr', type=float, default=1e-4)\n    parser.add_argument('--num_classes', type=int, default=2)\n    parser.add_argument('--seed', type=int, default=42)\n\n    parser.add_argument('--in_channels', type=int, default=3)\n    parser.add_argument('--input_size', type=tuple, default=(4, 3, 224, 224))\n    parser.add_argument('--target_size', type=tuple, default=(566, 640))\n    parser.add_argument('--mean', type=list, default=IMAGENET_DEFAULT_MEAN)\n    parser.add_argument('--std', type=list, default=IMAGENET_DEFAULT_STD)\n    \n    parser.add_argument('--project', type=str, default=\"MIA_HW4_own\")\n    parser.add_argument('--entity', type=str, default=\"DDCVLAB\")\n    parser.add_argument('--name', type=str, default=\"FCN\")\n    parser.add_argument('--wandb', type=bool, default=False)\n    parser.add_argument('--sweep', type=bool, default=True)\n    parser.add_argument('--sweep_counts', type=int, default=12)\n    \n    parser.add_argument('--output_dir', type=str, default=\"\")\n    parser.add_argument('--data_path', type=str, default=\"/kaggle/input/mia-hw4/sono/train/pre/*\")\n\n    args, _ = parser.parse_known_args()\n    return args\n\nsweep_config = {\n    \"program\": \"main.py\",\n    \"method\": \"random\", # [\"random\", \"grid\", \"bayes\"]\n    \"metrics\": {'goal': 'maximize', 'name': 'Validation Dice'},\n    \"parameters\": {\n        \"lr\": {\n            \"values\": [1e-3, 5e-4, 1e-4, 5e-5]\n        },\n        \"weight_decay\": {\n            \"values\": [1e-2, 1e-3, 1e-4, 1e-5, 1e-6, 1e-7, 1e-8]\n        },\n    }\n}","metadata":{"execution":{"iopub.status.busy":"2023-12-12T06:17:47.766079Z","iopub.execute_input":"2023-12-12T06:17:47.766496Z","iopub.status.idle":"2023-12-12T06:17:47.782859Z","shell.execute_reply.started":"2023-12-12T06:17:47.766409Z","shell.execute_reply":"2023-12-12T06:17:47.781205Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## FCN main","metadata":{}},{"cell_type":"code","source":"def sweeper(wandb_logger, args):\n    \n    if wandb_logger:\n        wandb_logger.init_run()\n        args = wandb_logger._wandb.config\n    \n    seed_everything(args.seed)\n    \n    img_files = glob.glob(args.data_path)\n    \n    train_ids, valid_ids = train_test_split(range(len(img_files)), test_size=0.2, random_state=args.seed)\n    train_transform, valid_transform = build_transform(True, args), build_transform(False, args)\n    train_dataset = DeepMedical(images=img_files, transforms=train_transform)\n    valid_dataset = DeepMedical(images=img_files, transforms=valid_transform)\n    train_dataset = SonoDataset(train_dataset, train_ids)\n    valid_dataset = SonoDataset(valid_dataset, valid_ids)\n    \n    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=args.batch_size)\n    valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=1)\n\n    model = FCN()\n    model.to(args.device)\n    if wandb_logger:\n        wandb_logger._wandb.watch(model)\n\n    params_to_optimize = [\n        {\"params\": [p for p in model.parameters() if p.requires_grad]},\n    ]\n    #TO DO: try to select a proper optimizer\n    optimizer = torch.optim.Adam(\n        params_to_optimize,\n        lr=args.lr, weight_decay=args.weight_decay\n    )\n    criterion = nn.CrossEntropyLoss()\n        \n    n_parameters = sum(p.numel() for p in params_to_optimize[0][\"params\"])\n    macs = profile_macs(model, torch.randn(args.input_size).to(args.device))\n    \n    print(\"FCN\")\n    print(\"Training augmentation:\", train_transform)\n    print('number of params (M): %.2f' % (n_parameters / 1.e6))\n    print('model flops (G):', macs / 2 / 1.e9, 'input_size:', args.input_size)\n    print(\"LR = %.8f\" % args.lr)\n    print(\"WD = %.8f\" % args.weight_decay)\n    print(\"Batch size = %d\" % args.batch_size)\n    print(\"Number of training epochs = %d\" % args.epochs)\n    print(\"Number of training examples = %d\" % len(train_dataset))\n    print(\"Number of training steps per epoch = %d\" % (len(train_dataset) // args.batch_size))\n    print(\"Use Cosine LR scheduler\")\n    print(\"criterion = %s\" % str(criterion))\n    \n    best_dice = 0\n    start_time = time.time()\n    print(\"Start Training\")\n    for epoch in range(args.epochs):\n\n        train_one_epoch(model, train_loader, criterion, args.device, epoch, optimizer)\n        train_logger, train_confmat = evaluate(model, train_loader, criterion, device=args.device, num_classes=args.num_classes, print_freq=len(train_dataset), header=\"Eval [Train]\")\n        valid_logger, valid_confmat = evaluate(model, valid_loader, criterion, device=args.device, num_classes=args.num_classes, print_freq=len(valid_dataset), header=\"Eval [Valid]\")\n        \n        if wandb_logger:\n            wandb_logger._wandb.log({f\"Training Loss\": getattr(train_logger, \"loss\", -1).value})\n            wandb_logger._wandb.log({f\"Training mIoU\": getattr(train_logger, \"mIoU\", -1).value})\n            wandb_logger._wandb.log({f\"Training Dice\": getattr(train_logger, \"Dice\", -1).value})\n\n            wandb_logger._wandb.log({f\"Validation Loss\": getattr(valid_logger, \"loss\", -1).value})\n            wandb_logger._wandb.log({f\"Validation mIoU\": getattr(valid_logger, \"mIoU\", -1).value})\n            wandb_logger._wandb.log({f\"Validation Dice\": getattr(valid_logger, \"Dice\", -1).value})\n        \n        info = str(valid_confmat)\n        print(info)\n\n        save_file = {\n            \"model\": model.state_dict(),\n            \"optimizer\": optimizer.state_dict(),\n            \"epoch\": epoch,\n            \"args\": wandb_logger.config if wandb_logger else {key: value for key, value in args.__dict__.items() if not key.startswith('__') and not callable(value)}\n        }\n        if valid_confmat.dice > best_dice:\n            best_dice = valid_confmat.dice\n            torch.save(save_file, os.path.join(args.output_dir, \"best_ckpt_FCN.pth\"))\n            if wandb_logger:\n                wandb_logger._wandb.save(\"best_ckpt_FCN.pth\")\n        torch.save(save_file, os.path.join(args.output_dir, \"last_ckpt_FCN.pth\"))\n        if wandb_logger:\n            wandb_logger._wandb.save(\"last_ckpt_FNC.pth\")\n        \n    if wandb_logger:\n        wandb_logger._wandb.log({f\"Best Dice\": best_dice})\n            \n    total_time = time.time() - start_time\n    total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n    print(\"Training finish with time {}\".format(total_time_str))\n    \n    if wandb_logger:\n        print(\"Start logging images\")\n        iterator = visualization(model, valid_loader, args)\n        for data in iterator:\n            wandb_logger.log_image(*data)\n        wandb_logger._wandb.log({\"Image table\": wandb_logger.mask_table})\n    print(\"Finish\")\n\nif __name__ == '__main__':\n    \n    print_with_timestamp()\n    \n    args = get_args_FCN()\n    config = {key: value for key, value in args.__dict__.items() if not key.startswith('__') and not callable(value)}\n    \n    if args.wandb:\n        from kaggle_secrets import UserSecretsClient\n        user_secrets = UserSecretsClient()\n\n        WANDB_API = user_secrets.get_secret(\"wandb\")\n        wandb_logger = WandbLogger(config)\n        wandb_logger.login(WANDB_API)\n    \n        if args.sweep:\n            sweep_id = wandb.sweep(sweep_config, project=args.project)\n            wandb.agent(sweep_id, function=lambda: sweeper(wandb_logger,args), count=args.sweep_counts)\n        else:\n            sweeper(wandb_logger, args)\n    else:\n        sweeper(None, args)","metadata":{"execution":{"iopub.status.busy":"2023-12-12T06:17:47.785062Z","iopub.execute_input":"2023-12-12T06:17:47.785573Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## UNet args","metadata":{}},{"cell_type":"code","source":"#TO DO: try to set a more fitness parameters\ndef get_args_UNet():\n    parser = argparse.ArgumentParser(description='SonoDataset Training')\n\n    parser.add_argument('--root', type=str, default=\"/kaggle/input/mia-hw4/sono\")\n    parser.add_argument('--device', type=str, default=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    parser.add_argument('--batch_size', type=int, default=2)\n    parser.add_argument('--weight_decay', type=float, default=1e-7)\n    parser.add_argument('--epochs', type=int, default=70)\n    parser.add_argument('--lr', type=float, default=5e-4)\n    parser.add_argument('--num_classes', type=int, default=2)\n    parser.add_argument('--seed', type=int, default=42)\n\n    parser.add_argument('--in_channels', type=int, default=3)\n    parser.add_argument('--input_size', type=tuple, default=(4, 3, 224, 224))\n    parser.add_argument('--target_size', type=tuple, default=(566, 640))\n    parser.add_argument('--mean', type=list, default=IMAGENET_DEFAULT_MEAN)\n    parser.add_argument('--std', type=list, default=IMAGENET_DEFAULT_STD)\n    \n    parser.add_argument('--project', type=str, default=\"MIA-HW4\")\n    parser.add_argument('--entity', type=str, default=\"DDCVLAB\")\n    parser.add_argument('--name', type=str, default=\"Unet\")\n    parser.add_argument('--wandb', type=bool, default=False)\n    parser.add_argument('--sweep', type=bool, default=True)\n    parser.add_argument('--sweep_counts', type=int, default=12)\n    \n    parser.add_argument('--output_dir', type=str, default=\"\")\n    parser.add_argument('--data_path', type=str, default=\"/kaggle/input/mia-hw4/sono/train/pre/*\")\n\n    args, _ = parser.parse_known_args()\n    return args\n\nsweep_config = {\n    \"program\": \"main.py\",\n    \"method\": \"random\", # [\"random\", \"grid\", \"bayes\"]\n    \"metrics\": {'goal': 'maximize', 'name': 'Validation Dice'},\n    \"parameters\": {\n        \"lr\": {\n            \"values\": [1e-3, 5e-4, 1e-4, 5e-5,1e-5]\n        },\n        \"weight_decay\": {\n            \"values\": [1e-2, 1e-3, 1e-4, 1e-5, 1e-6, 1e-7, 1e-8]\n        },\n    }\n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## UNet main","metadata":{}},{"cell_type":"code","source":"def sweeper(wandb_logger, args):\n    \n    if wandb_logger:\n        wandb_logger.init_run()\n        args = wandb_logger._wandb.config\n    \n    seed_everything(args.seed)\n    \n    img_files = glob.glob(args.data_path)\n    \n    train_ids, valid_ids = train_test_split(range(len(img_files)), test_size=0.2, random_state=args.seed)\n    train_transform, valid_transform = build_transform(True, args), build_transform(False, args)\n    train_dataset = DeepMedical(images=img_files, transforms=train_transform)\n    valid_dataset = DeepMedical(images=img_files, transforms=valid_transform)\n    train_dataset = SonoDataset(train_dataset, train_ids)\n    valid_dataset = SonoDataset(valid_dataset, valid_ids)\n    \n    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=args.batch_size)\n    valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=1)\n\n    model = UNet()\n    model.to(args.device)\n    if wandb_logger:\n        wandb_logger._wandb.watch(model)\n\n    params_to_optimize = [\n        {\"params\": [p for p in model.parameters() if p.requires_grad]},\n    ]\n    #TO DO: try to select a proper optimizer\n    optimizer = torch.optim.Adam(\n        params_to_optimize,\n        lr=args.lr, weight_decay=args.weight_decay\n    )\n    criterion = nn.CrossEntropyLoss()\n        \n    n_parameters = sum(p.numel() for p in params_to_optimize[0][\"params\"])\n    macs = profile_macs(model, torch.randn(args.input_size).to(args.device))\n    \n    print(\"UNet\")\n    print(\"Training augmentation:\", train_transform)\n    print('number of params (M): %.2f' % (n_parameters / 1.e6))\n    print('model flops (G):', macs / 2 / 1.e9, 'input_size:', args.input_size)\n    print(\"LR = %.8f\" % args.lr)\n    print(\"WD = %.8f\" % args.weight_decay)\n    print(\"Batch size = %d\" % args.batch_size)\n    print(\"Number of training epochs = %d\" % args.epochs)\n    print(\"Number of training examples = %d\" % len(train_dataset))\n    print(\"Number of training steps per epoch = %d\" % (len(train_dataset) // args.batch_size))\n    print(\"Use Cosine LR scheduler\")\n    print(\"criterion = %s\" % str(criterion))\n    \n    best_dice = 0\n    start_time = time.time()\n    print(\"Start Training\")\n    for epoch in range(args.epochs):\n\n        train_one_epoch(model, train_loader, criterion, args.device, epoch, optimizer)\n        train_logger, train_confmat = evaluate(model, train_loader, criterion, device=args.device, num_classes=args.num_classes, print_freq=len(train_dataset), header=\"Eval [Train]\")\n        valid_logger, valid_confmat = evaluate(model, valid_loader, criterion, device=args.device, num_classes=args.num_classes, print_freq=len(valid_dataset), header=\"Eval [Valid]\")\n        \n        if wandb_logger:\n            wandb_logger._wandb.log({f\"Training Loss\": getattr(train_logger, \"loss\", -1).value})\n            wandb_logger._wandb.log({f\"Training mIoU\": getattr(train_logger, \"mIoU\", -1).value})\n            wandb_logger._wandb.log({f\"Training Dice\": getattr(train_logger, \"Dice\", -1).value})\n\n            wandb_logger._wandb.log({f\"Validation Loss\": getattr(valid_logger, \"loss\", -1).value})\n            wandb_logger._wandb.log({f\"Validation mIoU\": getattr(valid_logger, \"mIoU\", -1).value})\n            wandb_logger._wandb.log({f\"Validation Dice\": getattr(valid_logger, \"Dice\", -1).value})\n        \n        info = str(valid_confmat)\n        print(info)\n\n        save_file = {\n            \"model\": model.state_dict(),\n            \"optimizer\": optimizer.state_dict(),\n            \"epoch\": epoch,\n            \"args\": wandb_logger.config if wandb_logger else {key: value for key, value in args.__dict__.items() if not key.startswith('__') and not callable(value)}\n        }\n        if valid_confmat.dice > best_dice:\n            best_dice = valid_confmat.dice\n            torch.save(save_file, os.path.join(args.output_dir, \"best_ckpt_UNet.pth\"))\n            if wandb_logger:\n                wandb_logger._wandb.save(\"best_ckpt_UNet.pth\")\n        torch.save(save_file, os.path.join(args.output_dir, \"last_ckpt_UNet.pth\"))\n        if wandb_logger:\n            wandb_logger._wandb.save(\"last_ckpt_UNet.pth\")\n        \n    if wandb_logger:\n        wandb_logger._wandb.log({f\"Best Dice\": best_dice})\n            \n    total_time = time.time() - start_time\n    total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n    print(\"Training finish with time {}\".format(total_time_str))\n    \n    if wandb_logger:\n        print(\"Start logging images\")\n        iterator = visualization(model, valid_loader, args)\n        for data in iterator:\n            wandb_logger.log_image(*data)\n        wandb_logger._wandb.log({\"Image table\": wandb_logger.mask_table})\n    print(\"UNet Finish\")\n\nif __name__ == '__main__':\n    \n    print_with_timestamp()\n    \n    args = get_args_UNet()\n    config = {key: value for key, value in args.__dict__.items() if not key.startswith('__') and not callable(value)}\n    \n    if args.wandb:\n        from kaggle_secrets import UserSecretsClient\n        user_secrets = UserSecretsClient()\n\n        WANDB_API = user_secrets.get_secret(\"wandb\")\n        wandb_logger = WandbLogger(config)\n        wandb_logger.login(WANDB_API)\n    \n        if args.sweep:\n            sweep_id = wandb.sweep(sweep_config, project=args.project)\n            wandb.agent(sweep_id, function=lambda: sweeper(wandb_logger,args), count=args.sweep_counts)\n        else:\n            sweeper(wandb_logger, args)\n    else:\n        sweeper(None, args)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Inference","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\nclass SonoInferenceDataset(torch.utils.data.Dataset):\n    \n    def __init__(self, root, df, transforms=None):\n        \n        self.root = root\n        self.df = df\n        self.transforms = transforms\n        \n    def __len__(self):\n        return self.df.shape[0]\n    \n    def __getitem__(self,idx):\n        \n        image_path = os.path.join(self.root, \"test\", \"pre\", self.df.iloc[idx,0])\n        \n        image = np.array(Image.open(image_path).convert('RGB'))\n\n        if self.transforms:\n            transformed = self.transforms(image=image)\n            image = transformed['image']\n            \n        return image\n\n@torch.no_grad()\ndef inference(model, data_loader, args):\n    model.eval()\n    \n    rle = []\n    for images in tqdm(data_loader):\n        images = images.to(device=args.device, dtype=torch.float)\n            \n        logits = model(images)\n        masks = logits.argmax(dim=1).detach().cpu().numpy()\n        for batch, mask in enumerate(masks):\n            mask = Image.fromarray((mask*255).astype(np.uint8)).resize((args.target_size[1], args.target_size[0]))\n            rle.append(rle_encode(np.array(mask), fg_val=255))\n    return rle","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## FCN test","metadata":{}},{"cell_type":"code","source":"args = get_args_FCN()\nsample_submission = pd.read_csv(os.path.join(args.root, \"sample_submission.csv\"))\ntest_dataset = SonoInferenceDataset(args.root, sample_submission, build_transform(False, args))\ntest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False)\nstate_dict = torch.load(os.path.join(args.output_dir, \"best_ckpt_FCN.pth\"), map_location=args.device)\nmodel = FCN()\nmodel.to(args.device)\nmodel.load_state_dict(state_dict[\"model\"])\nrle_FCN = inference(model, test_loader, args)\nsample_submission[\"rle_encode\"] = [list_to_string(r) for r in rle_FCN]\nsample_submission.to_csv(\"FCN-8s.csv\", index=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.imshow(rle_decode(list_to_string(rle_FCN[0])))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## UNet test","metadata":{}},{"cell_type":"code","source":"args = get_args_UNet()\nsample_submission = pd.read_csv(os.path.join(args.root, \"sample_submission.csv\"))\ntest_dataset = SonoInferenceDataset(args.root, sample_submission, build_transform(False, args))\ntest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False)\nstate_dict = torch.load(os.path.join(args.output_dir, \"best_ckpt_UNet.pth\"), map_location=args.device)\nmodel = UNet()\nmodel.to(args.device)\nmodel.load_state_dict(state_dict[\"model\"])\nrle_UNet = inference(model, test_loader, args)\nsample_submission[\"rle_encode\"] = [list_to_string(r) for r in rle_UNet]\nsample_submission.to_csv(\"U-Net.csv\", index=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.imshow(rle_decode(list_to_string(rle_UNet[0])))","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}