{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":6993660,"sourceType":"datasetVersion","datasetId":3917417}],"dockerImageVersionId":30587,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install pydicom","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-11-27T02:14:48.399922Z","iopub.execute_input":"2023-11-27T02:14:48.400644Z","iopub.status.idle":"2023-11-27T02:15:01.018445Z","shell.execute_reply.started":"2023-11-27T02:14:48.400609Z","shell.execute_reply":"2023-11-27T02:15:01.017337Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\n\nimport os\nimport numpy as np\nimport pandas as pd\nimport cv2\nimport pydicom\nimport random\nfrom collections import Counter, defaultdict\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nimport torchvision.models as models\nfrom torch.utils.data import Subset\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score\nfrom torch.nn.utils import clip_grad_norm_","metadata":{"execution":{"iopub.status.busy":"2023-11-27T02:15:01.020613Z","iopub.execute_input":"2023-11-27T02:15:01.020912Z","iopub.status.idle":"2023-11-27T02:15:05.967156Z","shell.execute_reply.started":"2023-11-27T02:15:01.020885Z","shell.execute_reply":"2023-11-27T02:15:05.966410Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2023-11-27T02:15:05.968342Z","iopub.execute_input":"2023-11-27T02:15:05.968770Z","iopub.status.idle":"2023-11-27T02:15:05.973329Z","shell.execute_reply.started":"2023-11-27T02:15:05.968743Z","shell.execute_reply":"2023-11-27T02:15:05.972121Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Interpolate():\n    \n    def __init__(self, target_num = 22):\n        self.target_num = target_num\n    \n    def __call__(self, slices): # (3, XX, 512, 512)\n        slices = slices.unsqueeze(dim = 0) # (1, 3, XX, 512, 512)\n        slices = F.interpolate(\n                  slices, \n                  mode = 'trilinear',\n                  size = (self.target_num, slices.shape[-2], slices.shape[-1])) # (1, 3, 22, 512, 512)\n        return slices.squeeze() # (slices.squeeze(): (3, 22, 512, 512))\n\n# transform\nall_train_transform = [\n    Interpolate(target_num = 22)\n]\n\ntrain_transform = transforms.Compose(all_train_transform)","metadata":{"execution":{"iopub.status.busy":"2023-11-27T02:15:05.974621Z","iopub.execute_input":"2023-11-27T02:15:05.974920Z","iopub.status.idle":"2023-11-27T02:15:05.988732Z","shell.execute_reply.started":"2023-11-27T02:15:05.974893Z","shell.execute_reply":"2023-11-27T02:15:05.987963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3D dataset","metadata":{}},{"cell_type":"code","source":"class MRI3DDataset(Dataset):\n    def __init__(self, df, transforms = None):\n        \n        root = \"/kaggle/input/hw3-data/hwk03_data/hwk03_data\"\n        self.ids = np.array(df[\"ID\"])\n        self.labels = torch.tensor(df[\"Disease\"])\n        self.transforms = transforms\n        \n        paths = []\n        for ID in self.ids:\n            path = os.path.join(root, \"DICOM\", str(ID).split('.')[0].zfill(7))\n            paths.append(path)\n        \n        self.images = []\n        \n        for path in paths:\n            all_slices = []\n            T1_root = os.path.join(root, \"DICOM\", path, \"T1\")\n            for filename in sorted(os.listdir(T1_root), key = lambda s: int(pydicom.dcmread(os.path.join(T1_root, s)).InstanceNumber)):\n                T1_image = []\n                T2_image = []\n                \n                # T1 images\n                img = pydicom.dcmread(os.path.join(T1_root, filename))\n                ww = img.WindowWidth\n                wc = img.WindowCenter\n                highest_visible_value = (ww + 2 * wc) / 2\n                lowest_visible_value = highest_visible_value - ww\n                \n                img = img.pixel_array\n                img = np.clip(img, lowest_visible_value, highest_visible_value)\n                img = 255 * (img - lowest_visible_value) / ww\n                img = cv2.resize(img, (512, 512))\n                \n                T1_image.append(img)\n                \n                # T2 images\n                img = pydicom.dcmread(os.path.join(root, \"DICOM\", path, \"T2\", filename))\n                ww = img.WindowWidth\n                wc = img.WindowCenter\n                highest_visible_value = (ww + 2 * wc) / 2\n                lowest_visible_value = highest_visible_value - ww\n                \n                img = img.pixel_array\n                img = np.clip(img, lowest_visible_value, highest_visible_value)\n                img = 255 * (img - lowest_visible_value) / ww\n                img = cv2.resize(img, (512, 512))\n                \n                T2_image.append(img)\n                \n                # T1T2 images\n                T1T2_image = (T1_image[0] + T2_image[0]) / 2\n                T1T2_image = torch.tensor(T1T2_image)\n                \n                T1_image = torch.tensor(T1_image[0])\n                T2_image = torch.tensor(T2_image[0])\n                \n                all_slices.append(torch.stack((T1_image, T2_image, T1T2_image), dim = 0))\n                \n            slices = torch.stack(all_slices, dim = 1)\n            \n            if self.transforms:\n                slices = self.transforms(slices) # (22, 3, 512, 512)\n            \n            self.images.append(slices.float())\n    \n    def __getitem__(self, index):\n        single = self.images[index]\n        label = self.labels[index]\n        \n        return single, label\n    \n    def __len__(self):\n        return(len(self.ids)) ","metadata":{"execution":{"iopub.status.busy":"2023-11-27T02:15:05.991134Z","iopub.execute_input":"2023-11-27T02:15:05.991661Z","iopub.status.idle":"2023-11-27T02:15:06.007635Z","shell.execute_reply.started":"2023-11-27T02:15:05.991636Z","shell.execute_reply":"2023-11-27T02:15:06.006801Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class config:\n    \n    root = '/kaggle/input/hw3-data/hwk03_data/hwk03_data'\n    valid_prob = 0.3\n        # train & test dataframe\n    #train_df = pd.read_csv(root+'/train.csv')\n    #test_df = pd.read_csv(root+'/test.csv')\n    #dataset_2D = MRI2DDataset(df = train_df, transforms = train_transform)\n    #test_2D = MRI2DDataset(df = test_df , transforms = train_transform)\n    #dataset_3D = MRI3DDataset(df = train_df, transforms = train_transform)\n    #test_3D = MRI3DDataset(df = test_df, transforms = train_transform)\n    \n    batch_size_single = 8\n    lr_single = 1e-4\n    epochs_single= 50\n    weight_decay_single = 1e-2\n    \n    batch_size_late = 8\n    lr_late = 1e-4\n    epochs_late= 50\n    weight_decay_late = 1e-3\n    \n    batch_size_early = 16\n    lr_early = 1e-3\n    epochs_early= 50\n    weight_decay_early = 1e-2\n    \n    batch_size_3D = 2\n    lr_3D = 1e-3\n    epochs_3D= 50\n    weight_decay_3D = 1e-2\n    \n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    seed = 42\n    \nprint(config.device)","metadata":{"execution":{"iopub.status.busy":"2023-11-27T02:15:06.008708Z","iopub.execute_input":"2023-11-27T02:15:06.009027Z","iopub.status.idle":"2023-11-27T02:15:06.088061Z","shell.execute_reply.started":"2023-11-27T02:15:06.008995Z","shell.execute_reply":"2023-11-27T02:15:06.087101Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def seed_everything(seed):\n    # Set Python random seed\n    random.seed(seed)\n    \n    # Set NumPy random seed\n    np.random.seed(seed)\n    \n    # Set PyTorch random seed for CPU and GPU\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n    \n    # Set PyTorch deterministic operations for cudnn backend\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\ndef evaluator(preds, gts):\n    preds = preds.cpu().numpy() if isinstance(preds, torch.Tensor) else preds\n    gts = gts.cpu().numpy() if isinstance(gts, torch.Tensor) else gts\n    acc = accuracy_score(preds, gts)\n    f1 = f1_score(preds, gts, average=\"macro\")\n\n    return acc, f1\ndef train_one_epoch_3D(model, train_loader, optimizer, scheduler, criterion, device):\n    model.train()\n    train_loss = 0.0\n    predictions, ground_truths = [], []\n    for batch_idx, (volumes, labels) in enumerate(train_loader):\n        # 將數據移動到指定的設備\n        volumes = volumes.to(device)\n        labels = labels.to(device)\n        optimizer.zero_grad()\n        logits = model(volumes)\n        loss = criterion(logits, labels)\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n        train_loss += loss.item()\n        preds = torch.argmax(logits, dim=1)\n\n        predictions.append(preds)\n        ground_truths.append(labels)\n\n    train_loss /= len(train_loader)\n\n    predictions = torch.cat(predictions)\n    ground_truths = torch.cat(ground_truths)\n    train_acc, train_f1 = evaluator(predictions, ground_truths)\n    torch.cuda.empty_cache()\n\n    return train_loss, 100*train_acc, 100*train_f1\n\ndef validation_3D(model, valid_loader, criterion, device):\n    model.eval()\n    valid_loss = 0.0\n    predictions = []\n    ground_truths = []\n    with torch.no_grad():\n        for batch_idx, (volumes, labels) in enumerate(valid_loader):\n            # 將數據移動到指定的設備\n            volumes = volumes.to(device)\n            labels = labels.to(device)\n\n            # 如果需要，你可能需要修改模型的輸入（根據你的模型結構）\n            logits = model(volumes)\n\n            loss = criterion(logits, labels)\n            valid_loss += loss.item()\n\n            preds = torch.argmax(logits, dim=1)\n            predictions.append(preds)\n            ground_truths.append(labels)\n\n        valid_loss /= len(valid_loader)\n\n        predictions = torch.cat(predictions)\n        ground_truths = torch.cat(ground_truths)\n        valid_acc, valid_f1 = evaluator(predictions, ground_truths)\n\n    return valid_loss, 100 * valid_acc, 100 * valid_f1\n\ndef test_3D(model, test_loader, device):\n    model.eval()\n    predictions_pro = []\n    predictions_stage = []\n\n    with torch.no_grad():\n        for batch_idx, (volumes, labels) in enumerate(test_loader):\n            # 將數據移動到指定的設備\n            volumes = volumes.to(device)\n            labels = labels.to(device)\n\n            # 如果需要，你可能需要修改模型的輸入（根據你的模型結構）\n            logits = model(volumes)  \n\n            pred_pro = nn.functional.softmax(logits, dim=1)\n            pred_stage = torch.argmax(pred_pro, dim=1)\n\n            predictions_pro.append(pred_pro.cpu().numpy())\n            predictions_stage.append((pred_stage.cpu().numpy()))\n\n    predictions_pro = np.concatenate(predictions_pro, axis=0)\n    predictions_stage = np.concatenate(predictions_stage, axis=0)\n\n    return predictions_pro, predictions_stage\n","metadata":{"execution":{"iopub.status.busy":"2023-11-27T02:15:06.089413Z","iopub.execute_input":"2023-11-27T02:15:06.089740Z","iopub.status.idle":"2023-11-27T02:15:06.109084Z","shell.execute_reply.started":"2023-11-27T02:15:06.089712Z","shell.execute_reply":"2023-11-27T02:15:06.108240Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3D model\n[MedicalNet](https://github.com/Tencent/MedicalNet.git)","metadata":{}},{"cell_type":"code","source":"# 載入github資源\n!git clone https://github.com/Tencent/MedicalNet.git\nimport sys\nsys.path.append('/kaggle/working/MedicalNet')\nfrom models import resnet","metadata":{"execution":{"iopub.status.busy":"2023-11-27T02:15:06.110123Z","iopub.execute_input":"2023-11-27T02:15:06.110459Z","iopub.status.idle":"2023-11-27T02:15:10.043179Z","shell.execute_reply.started":"2023-11-27T02:15:06.110402Z","shell.execute_reply":"2023-11-27T02:15:10.042128Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\nprint('device = ', device)\nprint(torch.cuda.get_device_name(0))","metadata":{"execution":{"iopub.status.busy":"2023-11-27T02:15:10.044530Z","iopub.execute_input":"2023-11-27T02:15:10.044817Z","iopub.status.idle":"2023-11-27T02:15:10.069144Z","shell.execute_reply.started":"2023-11-27T02:15:10.044789Z","shell.execute_reply":"2023-11-27T02:15:10.068343Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_model(model_type='resnet', model_depth=50,\n                   input_W=224, input_H=224, input_D=224, resnet_shortcut='B',\n                   no_cuda=True, gpu_id=[0],\n                   pretrain_path = 'pretrain/resnet_50.pth',\n                   nb_class=1):\n\n    assert model_type in [\n        'resnet'\n    ]\n\n    if model_type == 'resnet':\n        assert model_depth in [10, 18, 34, 50, 101, 152, 200]\n\n    model = resnet.resnet50(sample_input_W=input_W, sample_input_H=input_H, sample_input_D=input_D, num_seg_classes = nb_class)\n    model.conv_seg = nn.Sequential(nn.AdaptiveAvgPool3d((1, 1, 1)), nn.Flatten())\n    \n    if not no_cuda:\n        if len(gpu_id) > 1:\n            model = model.cuda()\n            model = nn.DataParallel(model, device_ids=gpu_id)\n            net_dict = model.state_dict()\n        else:\n            import os\n            os.environ[\"CUDA_VISIBLE_DEVICES\"]=str(gpu_id[0])\n            model = model.cuda()\n            model = nn.DataParallel(model, device_ids=None)\n            net_dict = model.state_dict()\n    else:\n        net_dict = model.state_dict()\n\n    print('loading pretrained model {}'.format(pretrain_path))\n    pretrain = torch.load(pretrain_path)\n    pretrain_dict = {k: v for k, v in pretrain['state_dict'].items() if k in net_dict.keys()}\n\n    net_dict.update(pretrain_dict)\n\n    return model\n\nclass Resnet3d(nn.Module):\n    def __init__(self, num_classes):\n        super().__init__()\n        \n        backend = generate_model(model_type='resnet',\n                         input_W=224, input_H=224, input_D=224, resnet_shortcut='B',\n                         pretrain_path = '/kaggle/input/hw3-data/resnet_50.pth',\n                         nb_class=2)\n\n        backend.classifier = nn.Identity()\n        for param in backend.parameters():\n            param.requires_grad = True \n            \n        # 更改成 3 channel\n        temp = [param for param in backend.conv1.parameters()][0] # (64, 1, 7, 7, 7)\n        temp = torch.cat((temp, temp, temp), dim = 1) # (64, 3, 7, 7, 7)\n        backend.conv1 = nn.Conv3d(3, 64, kernel_size=(7, 7, 7), stride=(2, 2, 2), padding=(3, 3, 3), bias=False)\n        #for param in backend.conv1.parameters():\n            #param = temp\n        \n        backend.conv1.weight.data.copy_(temp)\n        \n\n\n        self.backend = backend\n        self.classifier = nn.Linear(2048, num_classes)\n\n    def forward(self, x):\n        output = self.backend(x)\n        outputs = self.classifier(output)\n        return outputs","metadata":{"execution":{"iopub.status.busy":"2023-11-27T02:15:10.070503Z","iopub.execute_input":"2023-11-27T02:15:10.070786Z","iopub.status.idle":"2023-11-27T02:15:10.085507Z","shell.execute_reply.started":"2023-11-27T02:15:10.070761Z","shell.execute_reply":"2023-11-27T02:15:10.084611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def Resnet3D_modeling():\n\n    seed_everything(config.seed)\n    train_df = pd.read_csv(config.root+'/train.csv')\n    test_df = pd.read_csv(config.root+'/test.csv')\n\n\n\n    # Dataset\n    print(\"Resnet3D \")\n    print(\"Initializing dataset...\")\n    dataset = MRI3DDataset(df = train_df, transforms = train_transform)\n    print(\"Initializing test_dataset...\")\n    test_dataset = MRI3DDataset(df = test_df, transforms = train_transform)\n    \n    \n    # split training & validation dataset \n    n = len(dataset)\n    valid_size = int(n * config.valid_prob)\n    train_ids , valid_ids = train_test_split(\n     np.linspace(0, n - 1, n).astype(\"int\"),\n     test_size = valid_size,\n     random_state = config.seed,\n    )\n    print(f'Number of samples in train_dataset: {Counter(dataset.labels[train_ids])}')\n    print(f'Number of samples in val_dataset: {Counter(dataset.labels[valid_ids])}')\n    # DataLoader\n    train_dataset = Subset(dataset, train_ids)\n    valid_dataset = Subset(dataset, valid_ids)\n    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=config.batch_size_3D, shuffle=True)\n    valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=config.batch_size_3D, shuffle=False)\n    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=config.batch_size_3D, shuffle=False)\n\n    # settings\n    print(\"Resnet3D training\")\n    print(\"Initializing model...\")\n    num_classes = len(Counter(dataset.labels[train_ids]))\n    model = Resnet3d(num_classes = 2)\n    model.to(config.device)\n    criterion = nn.CrossEntropyLoss().to(config.device)\n    optimizer = torch.optim.Adam(model.parameters(), lr = config.lr_3D, weight_decay = config.weight_decay_3D)\n    scheduler = torch.optim.lr_scheduler.OneCycleLR(\n        optimizer = optimizer,\n        epochs = config.epochs_3D,\n        steps_per_epoch = train_loader.__len__(),\n        max_lr = config.lr_3D,\n        anneal_strategy = 'cos'\n    )\n\n    # recordings\n    best_val_loss = float(\"inf\")\n    history = {\n      \"train\": {\n          \"loss\": [],\n          \"acc\": [],\n          \"f1\": []\n      },\n      \"valid\": {\n          \"loss\": [],\n          \"acc\": [],\n          \"f1\": []\n      },\n    }\n    \n    for epoch in range(config.epochs_3D):\n        train_loss, train_acc, train_f1 = train_one_epoch_3D(model, train_loader, optimizer, scheduler, criterion, config.device)\n        valid_loss, valid_acc, valid_f1 = validation_3D(model, valid_loader, criterion, config.device)\n        \n        # Log the loss and validation result\n        history[\"train\"][\"loss\"].append(train_loss)\n        history[\"train\"][\"acc\"].append(train_acc)\n        history[\"train\"][\"f1\"].append(train_f1)\n        history[\"valid\"][\"loss\"].append(valid_loss)\n        history[\"valid\"][\"acc\"].append(valid_acc)\n        history[\"valid\"][\"f1\"].append(valid_f1)\n\n        print(f'Epoch[{epoch+1}/{config.epochs_3D}], Train Loss: {train_loss:.7f}, Train Accuracy: {train_acc:.4f}%, Train F1: {train_f1:.4f}% | Valid Loss: {valid_loss:.7f}, Valid Accuracy: {valid_acc:.4f}%, Valid F1: {valid_f1:.4f}% | LR: {optimizer.state_dict()[\"param_groups\"][0][\"lr\"]:.6f}')\n\n        if valid_loss < best_val_loss:\n            save_file = {\n                \"model\": model.state_dict(),\n                \"optimizer\": optimizer.state_dict(),\n                \"scheduler\": scheduler.state_dict(),\n                \"epoch\": epoch,\n                \"args\": config\n            }\n            best_val_loss = valid_loss\n            torch.save(save_file, \"checkpoint_3D.pth\")\n            \n    best_ckpt = torch.load(\"checkpoint_3D.pth\", map_location=config.device)\n    model.load_state_dict(best_ckpt[\"model\"])\n\n    print(\"3D plot\")\n    plt.figure(figsize=(12, 5))\n    plt.subplot(1, 2, 1)\n    plt.plot(range(config.epochs_3D), history[\"train\"][\"loss\"], label='Training Loss')\n    plt.plot(range(config.epochs_3D), history[\"valid\"][\"loss\"], label='Validation Loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.title('Training and Validation Loss Curves')\n    plt.show()\n\n    plt.figure(figsize=(12, 5))\n    plt.subplot(1, 2, 1)\n    plt.plot(range(config.epochs_3D), history[\"train\"][\"acc\"], label='Training Acc')\n    plt.plot(range(config.epochs_3D), history[\"valid\"][\"acc\"], label='Validation Acc')\n    plt.xlabel('Epoch')\n    plt.ylabel('Acc')\n    plt.legend()\n    plt.title('Training and Validation Accuracy Curves')\n    plt.show()\n\n    plt.figure(figsize=(12, 5))\n    plt.subplot(1, 2, 1)\n    plt.plot(range(config.epochs_3D), history[\"train\"][\"f1\"], label='Training F1')\n    plt.plot(range(config.epochs_3D), history[\"valid\"][\"f1\"], label='Validation F1')\n    plt.xlabel('Epoch')\n    plt.ylabel('F1 score')\n    plt.legend()\n    plt.title('Training and Validation F1 Score Curves')\n    plt.show()\n    \n    test_prediction_pro, test_prediction_stage = test_3D(model, test_loader, config.device)\n    # 读取现有的test.csv文件\n    test_df = pd.read_csv('/kaggle/input/hw3-data/hwk03_data/hwk03_data/test.csv')\n\n    # 创建一个DataFrame包含预测结果\n    predictions = pd.DataFrame(test_prediction_pro, columns=[f'Disease({i})' for i in range(2)])\n    predictions['Disease'] = test_prediction_stage\n    predictions = predictions[['Disease(0)', 'Disease(1)', 'Disease']]\n    # 将预测结果添加到现有DataFrame中\n    test_df['Disease 0'] = predictions['Disease(0)']\n    test_df['Disease 1'] = predictions['Disease(1)']\n    test_df['Disease'] = predictions['Disease']\n    # 另存为新的CSV文件\n    test_df.to_csv('3D.csv', index=False)\n    print(\"3D save\")\n\n\n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2023-11-27T02:15:10.087007Z","iopub.execute_input":"2023-11-27T02:15:10.087530Z","iopub.status.idle":"2023-11-27T02:15:10.114924Z","shell.execute_reply.started":"2023-11-27T02:15:10.087496Z","shell.execute_reply":"2023-11-27T02:15:10.114032Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def main():\n   \n    Resnet3D_modeling()\n    \n    \nif __name__ == \"__main__\":\n    main()","metadata":{"execution":{"iopub.status.busy":"2023-11-27T02:15:10.115987Z","iopub.execute_input":"2023-11-27T02:15:10.116293Z"},"trusted":true},"execution_count":null,"outputs":[]}]}