{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7041801,"sourceType":"datasetVersion","datasetId":4051533},{"sourceId":7259559,"sourceType":"datasetVersion","datasetId":4207080},{"sourceId":7318183,"sourceType":"datasetVersion","datasetId":4233295}],"dockerImageVersionId":30588,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# To do\n\n這份檔案會講解以下幾個部分:\n\n* COCO format 資料讀取\n* Object detection 模型訓練與評估","metadata":{}},{"cell_type":"code","source":"# import libraries\n\n# basic\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport os\nimport random\nimport numpy as np\nimport pandas as pd\nfrom tqdm.notebook import tqdm\n\n# visualization\nimport cv2\nimport matplotlib.pyplot as plt\nfrom PIL import Image\n\n# PyTorch\nimport torch\nimport torch.nn as nn\nimport torchvision\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import models\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nfrom torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\nfrom torchvision.models.detection import RetinaNet\nfrom torchvision.models.detection.retinanet import RetinaNetClassificationHead\n\n\n# object detection\n!pip install pycocotools\nimport pycocotools\nfrom pycocotools.coco import COCO","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-01-09T08:41:38.164008Z","iopub.execute_input":"2024-01-09T08:41:38.164446Z","iopub.status.idle":"2024-01-09T08:41:58.142336Z","shell.execute_reply.started":"2024-01-09T08:41:38.164408Z","shell.execute_reply":"2024-01-09T08:41:58.141293Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"為了使用 COCO API 來評估模型成效，我們會需要用到以下五個檔案，以下為 source code：\n\n- https://raw.githubusercontent.com/pytorch/vision/main/references/detection/engine.py\n- https://raw.githubusercontent.com/pytorch/vision/main/references/detection/utils.py\n- https://raw.githubusercontent.com/pytorch/vision/main/references/detection/coco_utils.py\n- https://raw.githubusercontent.com/pytorch/vision/main/references/detection/coco_eval.py\n- https://raw.githubusercontent.com/pytorch/vision/main/references/detection/transforms.py","metadata":{}},{"cell_type":"code","source":"!wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/engine.py\n!wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/utils.py\n!wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/coco_utils.py\n!wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/coco_eval.py\n!wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/transforms.py\n    \nfrom engine import evaluate","metadata":{"execution":{"iopub.status.busy":"2024-01-09T08:41:58.144633Z","iopub.execute_input":"2024-01-09T08:41:58.145474Z","iopub.status.idle":"2024-01-09T08:42:03.954825Z","shell.execute_reply.started":"2024-01-09T08:41:58.145435Z","shell.execute_reply":"2024-01-09T08:42:03.953834Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class config:\n    \n    root = \"/kaggle/input/hw5-dataset-new\"\n    num_classes = 8\n    batch_size = 4\n    epochs = 100\n    weight_decay = 0.0005\n    lr = 0.005\n    momentum = 0.9\n    milestones = [20,40,60,80]\n    gamma = 0.1\n    seed = 42\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    categories = ['normal', 'aortic_curvature', 'aortic_atherosclerosis_calcification', \n                  'cardiac_hypertrophy', 'intercostal_pleural_thickening', 'lung_field_infiltration', \n                  'degenerative_joint_disease_of_the_thoracic_spine', 'scoliosis']","metadata":{"execution":{"iopub.status.busy":"2024-01-09T08:42:03.956213Z","iopub.execute_input":"2024-01-09T08:42:03.956529Z","iopub.status.idle":"2024-01-09T08:42:04.022994Z","shell.execute_reply.started":"2024-01-09T08:42:03.956498Z","shell.execute_reply":"2024-01-09T08:42:04.021736Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def seed_everything(seed):\n    # Set Python random seed\n    random.seed(seed)\n    \n    # Set NumPy random seed\n    np.random.seed(seed)\n    \n    # Set PyTorch random seed for CPU and GPU\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n    \n    # Set PyTorch deterministic operations for cudnn backend\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False","metadata":{"execution":{"iopub.status.busy":"2024-01-09T08:42:04.026275Z","iopub.execute_input":"2024-01-09T08:42:04.026757Z","iopub.status.idle":"2024-01-09T08:42:04.033383Z","shell.execute_reply.started":"2024-01-09T08:42:04.026720Z","shell.execute_reply":"2024-01-09T08:42:04.032436Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n\n# Read data information\n\n我們可以利用 `pycocotools` 這個套件來讀取 .json 檔案中的資料，此處簡單介紹我們所創建的 COCO dataset 所包含的三個部分： <br>\n( 更詳細的介紹可閱讀[這裡](https://docs.aws.amazon.com/rekognition/latest/customlabels-dg/md-coco-overview.html) )\n\n## Categories\n\n包含所有類別的 dictionary ( 不含 background ) ，每個 dictionary 中需要 2 個 key :\n\n* `id` : 類別編號\n* `name` : 類別名稱","metadata":{}},{"cell_type":"code","source":"annfile = config.root + \"/train.json\"\ncoco = COCO(annfile)\ncoco.cats","metadata":{"execution":{"iopub.status.busy":"2024-01-09T08:42:04.034603Z","iopub.execute_input":"2024-01-09T08:42:04.034972Z","iopub.status.idle":"2024-01-09T08:42:04.060798Z","shell.execute_reply.started":"2024-01-09T08:42:04.034941Z","shell.execute_reply":"2024-01-09T08:42:04.059920Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"annfile_test = config.root + \"/test.json\"\ncoco_test = COCO(annfile_test)\ncoco_test.cats\ncoco_test.loadImgs(80)\n","metadata":{"execution":{"iopub.status.busy":"2024-01-09T08:42:04.061821Z","iopub.execute_input":"2024-01-09T08:42:04.062097Z","iopub.status.idle":"2024-01-09T08:42:04.073283Z","shell.execute_reply.started":"2024-01-09T08:42:04.062074Z","shell.execute_reply":"2024-01-09T08:42:04.072352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Images\n\n影像相關資訊，一個 dictionary 含一張影像，內有 4 個 key :\n\n* `file_name` : 影像路徑\n* `height` : 影像高度\n* `width` : 影像寬度\n* `id` : 影像編號 ( unique  )","metadata":{}},{"cell_type":"code","source":"coco_test.loadImgs(80)","metadata":{"execution":{"iopub.status.busy":"2024-01-09T08:42:04.074240Z","iopub.execute_input":"2024-01-09T08:42:04.074486Z","iopub.status.idle":"2024-01-09T08:42:04.080662Z","shell.execute_reply.started":"2024-01-09T08:42:04.074464Z","shell.execute_reply":"2024-01-09T08:42:04.079735Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Annotations\n\n標註相關資訊，一個 dictionary 只包含一個 annotation ( bounding box ) ，共有 7 個 key :\n\n* `id` : 該 annotation 的編號\n* `image_id` : 該 bounding box 所屬影像的編號\n* `category_id` : 該 bounding box 所屬類別的編號\n* `bbox` : bounding box 的標註資訊，格式為 $[\\text{xmin}, \\text{ymin}, \\text{width}, \\text{height}]$。$\\text{xmin}$ 和 $\\text{ymin}$ 表示 bounding box 左上角在影像上的座標，$\\text{width}$ 和 $\\text{height}$ 則為 bounding box 的寬跟高\n* `area` : 每個 bounding box 所圍出的面積。\n* `iscrowd` : 是單一物件 ( 0 ) 或一組物件 ( 1 )。segmentation 時使用，此處直接設為 0 即可\n* `segmentation` : segmentation 時使用，可忽略","metadata":{}},{"cell_type":"code","source":"ann_ids = coco.getAnnIds(imgIds = 80)\ncoco.loadAnns(ann_ids)","metadata":{"execution":{"iopub.status.busy":"2024-01-09T08:42:04.081941Z","iopub.execute_input":"2024-01-09T08:42:04.082293Z","iopub.status.idle":"2024-01-09T08:42:04.092945Z","shell.execute_reply.started":"2024-01-09T08:42:04.082257Z","shell.execute_reply":"2024-01-09T08:42:04.091935Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ann_ids = coco_test.getAnnIds(imgIds = 80)\ncoco_test.loadAnns(ann_ids)","metadata":{"execution":{"iopub.status.busy":"2024-01-09T08:42:04.094178Z","iopub.execute_input":"2024-01-09T08:42:04.094864Z","iopub.status.idle":"2024-01-09T08:42:04.100792Z","shell.execute_reply.started":"2024-01-09T08:42:04.094833Z","shell.execute_reply":"2024-01-09T08:42:04.099895Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n\n# Data augmentation\n\n由於經過 transform 後，圖片中的 bounding box 的位置與大小也會跟著改變，因此要將 bounding box 也一起進行轉換。","metadata":{}},{"cell_type":"code","source":"def get_transforms(train = False):\n    \n    if train:\n        transform = A.Compose([\n            A.Resize(800, 800),\n            A.HorizontalFlip(p = 0.3),\n            A.RandomBrightnessContrast(p = 0.1),\n            A.ColorJitter(p = 0.1),\n            ToTensorV2()\n        ], bbox_params = A.BboxParams(format = \"coco\"))\n    else:\n        transform = A.Compose([\n            A.Resize(800, 800),\n            ToTensorV2()\n        ], bbox_params = A.BboxParams(format = \"coco\"))\n    \n    return transform","metadata":{"execution":{"iopub.status.busy":"2024-01-09T08:42:04.105195Z","iopub.execute_input":"2024-01-09T08:42:04.105472Z","iopub.status.idle":"2024-01-09T08:42:04.111992Z","shell.execute_reply.started":"2024-01-09T08:42:04.105449Z","shell.execute_reply":"2024-01-09T08:42:04.111107Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n\n# Dataset\n\n在 Dataset 的部分，我們需要回傳的東西有兩項：image 和 target。\n\nimage 與先前作業沒有太大差異，只有讀取方式有所不同。至於 target 則是一個 dictionary，裡面需包含 5 個 key：\n\n1. `boxes`：該影像中所有 bounding box 的標註，格式為 $[\\text{xmin}, \\text{ymin}, \\text{xmax}, \\text{ymax}]$。$\\text{xmin}$ 和 $\\text{ymin}$ 表示 bounding box 左上角在影像上的座標，$\\text{xmax}$ 和 $\\text{ymax}$ 則表示 bounding box 右下角在影像上的座標\n2. `labels`：每個 bounding box 所對應的疾病類別\n3. `image_id`：影像編號\n4. `area`：每個 bounding box 所圍出的面積。**若 bounding box 有經過 transform，一定要記得重新計算**\n5. `iscrowd`：是單一物件 ( 0 ) 或一組物件 ( 1 )。segmentation 時使用，此處直接設為 0 即可","metadata":{}},{"cell_type":"code","source":"class CXRDataset(Dataset):\n    \n    def __init__(self, root, split, transforms = None):\n        self.split = split\n        self.root = root\n        self.transforms = transforms\n        self.coco = COCO(os.path.join(config.root, f\"{self.split}.json\"))\n        self.ids = list(sorted(self.coco.imgs.keys()))\n        self.ids = [img_id for img_id in self.ids if (len(self.get_annotation(img_id)) > 0)]\n    \n    def get_image(self, img_id: int):\n        image_path = self.coco.loadImgs(img_id)[0]['file_name']\n        image = cv2.imread(os.path.join(self.root, image_path))\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        \n        return image\n    \n    def get_annotation(self, img_id: int):\n        annotation = self.coco.loadAnns(self.coco.getAnnIds(img_id))\n        \n        return annotation\n          \n    def __getitem__(self, index):\n        img_id = self.ids[index]\n        image = self.get_image(img_id)\n        annotation = self.get_annotation(img_id)\n        bboxes = [a['bbox'] + [a['category_id']] for a in annotation]\n        \n        if self.transforms:\n            transformed = self.transforms(image = image, bboxes = bboxes)\n        \n        image = transformed['image'] / 255\n        bboxes = transformed['bboxes']\n        \n        new_bboxes = list(map(lambda x: [x[0], x[1], x[0] + x[2], x[1] + x[3]], bboxes))\n        new_bboxes = torch.tensor(new_bboxes, dtype = torch.float32)\n        \n        target = {}\n        target['boxes'] = new_bboxes\n        target['labels'] = torch.tensor([a['category_id'] for a in annotation], dtype = torch.int64)\n        target['image_id'] = img_id\n        target['area'] = (new_bboxes[:, 3] - new_bboxes[:, 1]) * (new_bboxes[:, 2] - new_bboxes[:, 0])\n        target['iscrowd'] = torch.tensor([a['iscrowd'] for a in annotation], dtype = torch.int64)\n        \n        return image, target\n        \n    def __len__(self):\n        return len(self.ids)","metadata":{"execution":{"iopub.status.busy":"2024-01-09T08:42:04.113262Z","iopub.execute_input":"2024-01-09T08:42:04.113549Z","iopub.status.idle":"2024-01-09T08:42:04.129492Z","shell.execute_reply.started":"2024-01-09T08:42:04.113526Z","shell.execute_reply":"2024-01-09T08:42:04.128702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Collate_fn\n\n用於 dataloader。由於 object detection 讀取 data 的方式與先前的 classification 和 segmentation 有所不同，因此需自定義 `collate_fn`。 <br>\n此處有沒有加 `tuple()` 都沒關係。","metadata":{}},{"cell_type":"code","source":"def collate_fn(batch: list[torch.tensor, dict]):\n    return zip(*batch)\n","metadata":{"execution":{"iopub.status.busy":"2024-01-09T08:42:04.130606Z","iopub.execute_input":"2024-01-09T08:42:04.130897Z","iopub.status.idle":"2024-01-09T08:42:04.146168Z","shell.execute_reply.started":"2024-01-09T08:42:04.130851Z","shell.execute_reply":"2024-01-09T08:42:04.145183Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ntrain_dataset = CXRDataset(root = config.root, split = \"train\", transforms = get_transforms(train = True))\ntrain_loader = DataLoader(train_dataset, batch_size = config.batch_size, shuffle = False, collate_fn = collate_fn)\nprint(len(train_dataset))\nprint(len(train_loader))\n      ","metadata":{"execution":{"iopub.status.busy":"2024-01-09T08:42:04.147269Z","iopub.execute_input":"2024-01-09T08:42:04.147606Z","iopub.status.idle":"2024-01-09T08:42:04.162674Z","shell.execute_reply.started":"2024-01-09T08:42:04.147574Z","shell.execute_reply":"2024-01-09T08:42:04.161685Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n\n# Model: Faster R-CNN\n\n這邊使用 torchvision 中內建的 Faster R-CNN 模型，並加載預訓練權重，但要記得更改 predictor 的類別數量為 8 類 ( 含 background，也就是 normal ) ，如下所示： ","metadata":{}},{"cell_type":"code","source":"def fasterrcnn(num_classes):\n    model = models.detection.fasterrcnn_resnet50_fpn(pretrained = True)\n    in_features = model.roi_heads.box_predictor.cls_score.in_features\n    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n\n    return model","metadata":{"execution":{"iopub.status.busy":"2024-01-09T08:42:04.164014Z","iopub.execute_input":"2024-01-09T08:42:04.164337Z","iopub.status.idle":"2024-01-09T08:42:04.170852Z","shell.execute_reply.started":"2024-01-09T08:42:04.164307Z","shell.execute_reply":"2024-01-09T08:42:04.170056Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"模型架構如下：","metadata":{}},{"cell_type":"code","source":"model = fasterrcnn(num_classes = config.num_classes)\nprint(model)\n\ndel model","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2024-01-09T08:42:04.171855Z","iopub.execute_input":"2024-01-09T08:42:04.172260Z","iopub.status.idle":"2024-01-09T08:42:05.858846Z","shell.execute_reply.started":"2024-01-09T08:42:04.172223Z","shell.execute_reply":"2024-01-09T08:42:05.857290Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n\n# Training\n\n在 PyTorch 的 Faster R-CNN 這個模型中，我們不須再自行定義 loss function，因為在 `model.train()` 下，`model(images, targets)` 會自動回傳訓練的 loss，其包含以下四種損失：\n\n1. `loss_classifier`：分類器之損失\n2. `loss_box_reg`：bounding box regressor 之損失\n3. `loss_rpn_box_reg`：RPN 中 bounding box regressor 之損失\n4. `loss_objectness`：RPN 中分類器之損失，此分類器用以判斷 bounding box 中是否包含物體\n\n總損失為這四種 loss 的總和。","metadata":{}},{"cell_type":"code","source":"def train_one_epoch(model, train_loader, optimizer, scheduler, device):\n    model.train()\n    \n    train_loss = []\n    train_loss_dict = []\n    \n    for images, targets in tqdm(train_loader):\n        images = list(image.to(device) for image in images)\n        targets = [{k: torch.tensor(v).to(device) for k, v in t.items()} for t in targets]\n        \n        optimizer.zero_grad()\n        \n        loss = model(images, targets)\n        total_loss = sum(l for l in loss.values())\n        loss_value = total_loss.item()\n        loss_dict = {k: v.item() for k, v in loss.items()}\n        train_loss.append(loss_value)\n        train_loss_dict.append(loss_dict)\n        \n        total_loss.backward()\n        optimizer.step()\n    scheduler.step()\n        \n    train_loss = np.mean(train_loss)\n    \n    train_loss_dict = pd.DataFrame(train_loss_dict)\n    train_loss_classifier = train_loss_dict['loss_classifier'].mean()\n    train_loss_box_reg = train_loss_dict['loss_box_reg'].mean()\n    train_loss_rpn_box_reg = train_loss_dict['loss_rpn_box_reg'].mean()\n    train_loss_objectness = train_loss_dict['loss_objectness'].mean()\n    \n    return train_loss, train_loss_classifier, train_loss_box_reg, train_loss_rpn_box_reg, train_loss_objectness","metadata":{"execution":{"iopub.status.busy":"2024-01-09T08:42:05.860347Z","iopub.execute_input":"2024-01-09T08:42:05.860716Z","iopub.status.idle":"2024-01-09T08:42:05.870754Z","shell.execute_reply.started":"2024-01-09T08:42:05.860680Z","shell.execute_reply":"2024-01-09T08:42:05.869910Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n\n# Validation\n\n在此模型中，若設定 `model.eval()`，只會返回預測的 bounding box、confidence score 和該 bounding box 的 label。\n\n為了取得 validation set 的 loss 以選出最好的模型，這裡我在進行 validation 時使用 `model.train()`。如果要這麼做，需要把模型中的 batch normalization 和 dropout 的係數固定住，但因 Faster R-CNN 中不含 dropout 層，所以只需凍結 batch normalization 層的參數。","metadata":{}},{"cell_type":"code","source":"@torch.no_grad()\ndef validation(model, val_loader, device):\n    model.train()\n    \n    for m in model.modules():\n        if isinstance(m, torchvision.ops.Conv2dNormActivation):\n            m.eval()\n    \n    val_loss = []\n    val_loss_dict = []\n    \n    for images, targets in tqdm(val_loader):\n        images = list(image.to(device) for image in images)\n        targets = [{k: torch.tensor(v).to(device) for k, v in t.items()} for t in targets]\n        \n        loss = model(images, targets)\n        total_loss = sum(l for l in loss.values())\n        loss_value = total_loss.item()\n        loss_dict = {k: v.item() for k, v in loss.items()}\n        val_loss.append(loss_value)\n        val_loss_dict.append(loss_dict)\n    \n    val_loss = np.mean(val_loss)\n    \n    val_loss_dict = pd.DataFrame(val_loss_dict)\n    val_loss_classifier = val_loss_dict['loss_classifier'].mean()\n    val_loss_box_reg = val_loss_dict['loss_box_reg'].mean()\n    val_loss_rpn_box_reg = val_loss_dict['loss_rpn_box_reg'].mean()\n    val_loss_objectness = val_loss_dict['loss_objectness'].mean()\n    \n    return val_loss, val_loss_classifier, val_loss_box_reg, val_loss_rpn_box_reg, val_loss_objectness","metadata":{"execution":{"iopub.status.busy":"2024-01-09T08:42:05.872114Z","iopub.execute_input":"2024-01-09T08:42:05.872473Z","iopub.status.idle":"2024-01-09T08:42:05.883814Z","shell.execute_reply.started":"2024-01-09T08:42:05.872440Z","shell.execute_reply":"2024-01-09T08:42:05.882899Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Test","metadata":{}},{"cell_type":"code","source":"def predict(input_tensor, model, device):\n    outputs = model(input_tensor)\n    pred_classes = [config.categories[i] for i in outputs[0]['labels'].cpu().numpy()]\n    pred_labels = outputs[0]['labels'].cpu().numpy()\n    pred_scores = outputs[0]['scores'].detach().cpu().numpy()\n    pred_bboxes = outputs[0]['boxes'].detach().cpu().numpy()\n    \n    boxes, classes, labels, scores, indices = [], [], [], [], []\n    for index in range(len(pred_scores)):\n        # 不再檢查閾值，保留所有預測框\n        boxes.append(pred_bboxes[index].astype(np.int32))\n        classes.append(pred_classes[index])\n        labels.append(pred_labels[index])\n        scores.append(pred_scores[index])\n        indices.append(index)\n    \n    return boxes, classes, labels, scores, indices","metadata":{"execution":{"iopub.status.busy":"2024-01-09T08:42:05.884928Z","iopub.execute_input":"2024-01-09T08:42:05.885201Z","iopub.status.idle":"2024-01-09T08:42:05.896812Z","shell.execute_reply.started":"2024-01-09T08:42:05.885178Z","shell.execute_reply":"2024-01-09T08:42:05.895925Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n\n# Main","metadata":{}},{"cell_type":"code","source":"def main():\n    \n    seed_everything(config.seed)\n    \n    train_dataset = CXRDataset(root = config.root, split = \"train\", transforms = get_transforms(train = True))\n    val_dataset = CXRDataset(root = config.root, split = \"val\", transforms = get_transforms(train = False))\n\n    train_loader = DataLoader(train_dataset, batch_size = config.batch_size, shuffle = True, collate_fn = collate_fn)\n    val_loader = DataLoader(val_dataset, batch_size = config.batch_size, shuffle = False, collate_fn = collate_fn)\n    \n    device = config.device\n    model =  fasterrcnn(num_classes = config.num_classes)\n    model.to(device)\n    parameters = [p for p in model.parameters() if p.requires_grad]\n    optimizer = torch.optim.SGD(parameters, lr = config.lr, momentum = config.momentum, nesterov = True, weight_decay = config.weight_decay)\n    scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones = config.milestones, gamma = config.gamma)\n    #scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size = 5, gamma = config.gamma)\n    \n    best_val_loss = float(\"inf\")\n    history = {\n        \"train\": {\n            \"loss\": [],\n            \"loss_classifier\": [],\n            \"loss_box_reg\": [],\n            \"loss_rpn_box_reg\": [],\n            \"loss_objectness\": []\n        },\n        \"val\": {\n            \"loss\": [],\n            \"loss_classifier\": [],\n            \"loss_box_reg\": [],\n            \"loss_rpn_box_reg\": [],\n            \"loss_objectness\": []\n        },\n    }\n    \n    for epoch in range(config.epochs):\n        train_loss, train_loss_classifier, train_loss_box_reg, train_loss_rpn_box_reg, train_loss_objectness = train_one_epoch(model, train_loader, optimizer, scheduler, device)\n        val_loss, val_loss_classifier, val_loss_box_reg, val_loss_rpn_box_reg, val_loss_objectness = validation(model, val_loader, device)\n    \n        history[\"train\"][\"loss\"].append(train_loss)\n        history[\"train\"][\"loss_classifier\"].append(train_loss_classifier)\n        history[\"train\"][\"loss_box_reg\"].append(train_loss_box_reg)\n        history[\"train\"][\"loss_rpn_box_reg\"].append(train_loss_rpn_box_reg)\n        history[\"train\"][\"loss_objectness\"].append(train_loss_objectness)\n        \n        history[\"val\"][\"loss\"].append(val_loss)\n        history[\"val\"][\"loss_classifier\"].append(val_loss_classifier)\n        history[\"val\"][\"loss_box_reg\"].append(val_loss_box_reg)\n        history[\"val\"][\"loss_rpn_box_reg\"].append(val_loss_rpn_box_reg)\n        history[\"val\"][\"loss_objectness\"].append(val_loss_objectness)\n        \n        print(f'Epoch: {epoch+1}/{config.epochs} | LR: {optimizer.state_dict()[\"param_groups\"][0][\"lr\"]:.6f}')\n        print(\"*****Training*****\")\n        print(f'Loss: {train_loss:.4f} | Classifier Loss: {train_loss_classifier:.4f} | Box Reg Loss: {train_loss_box_reg:.4f} | RPN Box Reg Loss: {train_loss_rpn_box_reg:.4f} | Objectness Loss: {train_loss_objectness:.4f}')\n        evaluate(model, train_loader, device = device)\n        print(\"*****Validation*****\")\n        print(f'Loss: {val_loss:.4f} | Classifier Loss: {val_loss_classifier:.4f} | Box Reg Loss: {val_loss_box_reg:.4f} | RPN Box Reg Loss: {val_loss_rpn_box_reg:.4f} | Objectness Loss: {val_loss_objectness:.4f}')\n        evaluate(model, val_loader, device = device)\n        \n        if val_loss < best_val_loss:\n            save_file = {\n                \"model\": model.state_dict(),\n                \"optimizer\": optimizer.state_dict(),\n                \"scheduler\": scheduler.state_dict(),\n                \"epoch\": epoch,\n                \"args\": config\n            }\n            best_val_loss = val_loss\n            torch.save(save_file, \"checkpoint.pth\")\n            \n    best_ckpt = torch.load(\"checkpoint.pth\", map_location = device)\n    model.load_state_dict(best_ckpt[\"model\"])\n        \n    plt.figure(figsize = (12, 5))\n    plt.subplot(1, 2, 1)\n    plt.plot(range(config.epochs), history[\"train\"][\"loss\"], label = 'Training Loss')\n    plt.plot(range(config.epochs), history[\"val\"][\"loss\"], label = 'Validation Loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.title('Training and Validation Loss Curves')\n    plt.show()\n        \n    plt.figure(figsize = (12, 5))\n    plt.subplot(1, 2, 1)\n    plt.plot(range(config.epochs), history[\"train\"][\"loss_classifier\"], label = 'Training Classifier Loss')\n    plt.plot(range(config.epochs), history[\"val\"][\"loss_classifier\"], label = 'Validation Classifier Loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Classifier Loss')\n    plt.legend()\n    plt.title('Training and Validation Classifier Loss Curves')\n    plt.show()\n        \n    plt.figure(figsize = (12, 5))\n    plt.subplot(1, 2, 1)\n    plt.plot(range(config.epochs), history[\"train\"][\"loss_box_reg\"], label = 'Training Box Reg Loss')\n    plt.plot(range(config.epochs), history[\"val\"][\"loss_box_reg\"], label = 'Validation Box Reg Loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Box Reg Loss')\n    plt.legend()\n    plt.title('Training and Validation Box Reg Loss Curves')\n    plt.show()\n        \n    plt.figure(figsize = (12, 5))\n    plt.subplot(1, 2, 1)\n    plt.plot(range(config.epochs), history[\"train\"][\"loss_rpn_box_reg\"], label = 'Training RPN Box Reg Loss')\n    plt.plot(range(config.epochs), history[\"val\"][\"loss_rpn_box_reg\"], label = 'Validation RPN Box Reg Loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('RPN Box Reg Loss')\n    plt.legend()\n    plt.title('Training and Validation RPN Box Reg Loss Curves')\n    plt.show()\n        \n    plt.figure(figsize = (12, 5))\n    plt.subplot(1, 2, 1)\n    plt.plot(range(config.epochs), history[\"train\"][\"loss_objectness\"], label = 'Training Objectness Loss')\n    plt.plot(range(config.epochs), history[\"val\"][\"loss_objectness\"], label = 'Validation Objectness Loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Objectness Loss')\n    plt.legend()\n    plt.title('Training and Validation Objectness Loss Curves')\n    plt.show()\n    \n    print(\"Test data\")\n    \n    model.to(device)\n    transforms =  torchvision.transforms.Compose([ torchvision.transforms.ToTensor(), ])\n    # 假設 test.csv 文件有 'ID'、'Filename' 等列\n    test_df = pd.read_csv('/kaggle/input/hw5-dataset/hwk05_data/test.csv')\n    all_images = os.listdir(os.path.join(config.root, 'test'))\n    # 創建一個將檔案名映射到ID的字典\n    filename_to_id = dict(zip(test_df['Filename'], test_df['ID']))\n    predictions_list = []\n    for image_name in all_images:\n        image_path = os.path.join(config.root, 'test', image_name)\n        image = Image.open(image_path)\n        current_filename = f'{image_name.split(\".\")[0]}.dcm'\n        # 從字典中獲取對應的 ID\n        current_id = filename_to_id.get(current_filename, '未知')\n        # 為每個圖像初始化 prediction_dict\n        prediction_dict = {'ID': current_id, 'Filename': current_filename, 'predictions': []}\n        image = np.array(image)\n        image = cv2.resize(image, (800, 800))\n        image_float_np = np.float32(image) / 255\n        input_tensor = transforms(image)\n        input_tensor = input_tensor.to(config.device)\n        input_tensor = input_tensor.unsqueeze(0)\n        boxes, classes, labels, scores, indices = predict(input_tensor, model, config.device)\n        for box, cls, label, score, ind in zip(boxes, classes, labels, scores, indices):\n            xmin, ymin, xmax, ymax = box.tolist()\n            box_dict = {\n                'Class': cls,\n                'Label': label,\n                'Score': score,\n                'xmin': xmin/800,\n                'ymin': ymin/800,\n                'xmax': xmax/800,\n                'ymax': ymax/800\n            }\n            prediction_dict['predictions'].append(box_dict)\n        predictions_list.append(prediction_dict)\n    predictions_list = sorted(predictions_list, key=lambda x: x['Filename'])\n\n        # 將 predictions_list 轉換為 DataFrame\n    columns = ['ID', 'Category', 'Score', 'xmin', 'ymin', 'xmax', 'ymax']\n    data = []\n    for prediction in predictions_list:\n        current_id = prediction['ID']\n        for box_dict in prediction['predictions']:\n            current_class = box_dict['Class']\n            current_score = box_dict['Score']\n            xmin = box_dict['xmin']\n            ymin = box_dict['ymin']\n            xmax = box_dict['xmax']\n            ymax = box_dict['ymax']\n                \n            data.append([current_id, current_class, current_score, xmin, ymin, xmax, ymax])\n    result_df = pd.DataFrame(data, columns=columns)\n    # 將結果 DataFrame 保存為 CSV 檔\n    result_df.to_csv('fastrcnn.csv', index=False)\n    print(\"fastrcnn save\")\n\n ","metadata":{"execution":{"iopub.status.busy":"2024-01-09T08:42:05.898276Z","iopub.execute_input":"2024-01-09T08:42:05.898831Z","iopub.status.idle":"2024-01-09T08:42:05.935791Z","shell.execute_reply.started":"2024-01-09T08:42:05.898797Z","shell.execute_reply":"2024-01-09T08:42:05.934997Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    main()","metadata":{"execution":{"iopub.status.busy":"2024-01-09T08:42:05.936929Z","iopub.execute_input":"2024-01-09T08:42:05.937211Z","iopub.status.idle":"2024-01-09T08:44:11.237327Z","shell.execute_reply.started":"2024-01-09T08:42:05.937188Z","shell.execute_reply":"2024-01-09T08:44:11.236211Z"},"trusted":true},"execution_count":null,"outputs":[]}]}